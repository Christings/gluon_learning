{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器翻译\n",
    "\n",
    "本节介绍编码器—解码器和注意力机制的应用。我们以机器翻译为例，使用 Gluon 实现一个含注意力机制的编码器—解码器。机器翻译的输入与输出都是不定长的文本序列。\n",
    "\n",
    "\n",
    "## 含注意力机制的编码器—解码器\n",
    "\n",
    "首先，导入实现所需的包或模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import io\n",
    "import mxnet as mx\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.contrib import text\n",
    "from mxnet.gluon import data as gdata, loss as gloss, nn, rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面定义一些特殊符号。其中“&lt;pad&gt;”（padding）符号使每个序列等长；“&lt;bos&gt;”（beginning of sequence）符号表示序列的开始；而“&lt;eos&gt;”（end of sequence）符号表示序列的结束。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>'\n",
    "BOS = '<bos>'\n",
    "EOS = '<eos>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下设置了模型超参数。我们在编码器和解码器中分别使用了一层和两层的循环神经网络。实验中，我们选取长度不超过 5 的输入和输出序列，并将预测时输出序列的最大长度设为 20。这些序列长度考虑了句末添加的“&lt;eos&gt;”符号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 40\n",
    "eval_interval = 10\n",
    "lr = 0.005\n",
    "batch_size = 2\n",
    "max_seq_len = 5\n",
    "max_test_output_len = 20\n",
    "encoder_num_layers = 1\n",
    "decoder_num_layers = 2\n",
    "encoder_drop_prob = 0.1\n",
    "decoder_drop_prob = 0.1\n",
    "encoder_embed_size = 256\n",
    "encoder_num_hiddens = 256\n",
    "decoder_num_hiddens = 256\n",
    "alignment_size = 25\n",
    "ctx = mx.cpu(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据\n",
    "\n",
    "我们定义函数读取训练数据集。为了演示方便，这里使用了一个很小的法语—英语数据集。在读取数据时，我们在句末附上“&lt;eos&gt;”符号，并可能通过添加“&lt;pad&gt;”符号使每个序列等长。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(max_seq_len):\n",
    "    input_tokens = []\n",
    "    output_tokens = []\n",
    "    input_seqs = []\n",
    "    output_seqs = []\n",
    "    with io.open('../data/fr-en-small.txt') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            input_seq, output_seq = line.rstrip().split('\\t')\n",
    "            cur_input_tokens = input_seq.split(' ')\n",
    "            cur_output_tokens = output_seq.split(' ')\n",
    "            if len(cur_input_tokens) < max_seq_len and \\\n",
    "                    len(cur_output_tokens) < max_seq_len:\n",
    "                input_tokens.extend(cur_input_tokens)\n",
    "                # 句末附上 EOS 符号。\n",
    "                cur_input_tokens.append(EOS)\n",
    "                # 添加 PAD 符号使每个序列等长（长度为 max_seq_len ）。\n",
    "                while len(cur_input_tokens) < max_seq_len:\n",
    "                    cur_input_tokens.append(PAD)\n",
    "                input_seqs.append(cur_input_tokens)\n",
    "                output_tokens.extend(cur_output_tokens)\n",
    "                cur_output_tokens.append(EOS)\n",
    "                while len(cur_output_tokens) < max_seq_len:\n",
    "                    cur_output_tokens.append(PAD)\n",
    "                output_seqs.append(cur_output_tokens)\n",
    "        fr_vocab = text.vocab.Vocabulary(collections.Counter(input_tokens),\n",
    "                                         reserved_tokens=[PAD, BOS, EOS])\n",
    "        en_vocab = text.vocab.Vocabulary(collections.Counter(output_tokens),\n",
    "                                         reserved_tokens=[PAD, BOS, EOS])\n",
    "    return fr_vocab, en_vocab, input_seqs, output_seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下创建训练数据集。每一个样本包含法语的输入序列和英语的输出序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vocab, output_vocab, input_seqs, output_seqs = read_data(max_seq_len)\n",
    "fr = nd.zeros((len(input_seqs), max_seq_len), ctx=ctx)\n",
    "en = nd.zeros((len(output_seqs), max_seq_len), ctx=ctx)\n",
    "for i in range(len(input_seqs)):\n",
    "    fr[i] = nd.array(input_vocab.to_indices(input_seqs[i]), ctx=ctx)\n",
    "    en[i] = nd.array(output_vocab.to_indices(output_seqs[i]), ctx=ctx)\n",
    "dataset = gdata.ArrayDataset(fr, en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义编码器\n",
    "\n",
    "以下定义了基于门控循环单元的编码器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Block):\n",
    "    def __init__(self, num_inputs, embed_size, num_hiddens, num_layers,\n",
    "                 drop_prob, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.embedding = nn.Embedding(num_inputs, embed_size)\n",
    "            self.dropout = nn.Dropout(drop_prob)\n",
    "            self.rnn = rnn.GRU(num_hiddens, num_layers, dropout=drop_prob,\n",
    "                               input_size=embed_size)\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        embedding = self.embedding(inputs).swapaxes(0, 1)\n",
    "        embedding = self.dropout(embedding)\n",
    "        output, state = self.rnn(embedding, state)\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.rnn.begin_state(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义含注意力机制的解码器\n",
    "\n",
    "以下定义了基于门控循环单元的解码器。解码器中注意力机制的实现参考了 [“注意力机制”](attention.md) 一节中的描述。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Block):\n",
    "    def __init__(self, num_hiddens, num_outputs, num_layers, max_seq_len,\n",
    "                 drop_prob, alignment_size, encoder_num_hiddens, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.encoder_num_hiddens = encoder_num_hiddens\n",
    "        self.hidden_size = num_hiddens\n",
    "        self.num_layers = num_layers\n",
    "        with self.name_scope():\n",
    "            self.embedding = nn.Embedding(num_outputs, num_hiddens)\n",
    "            self.dropout = nn.Dropout(drop_prob)\n",
    "            # 注意力机制。\n",
    "            self.attention = nn.Sequential()\n",
    "            with self.attention.name_scope():\n",
    "                self.attention.add(\n",
    "                    nn.Dense(alignment_size,\n",
    "                             in_units=num_hiddens + encoder_num_hiddens,\n",
    "                             activation='tanh', flatten=False))\n",
    "                self.attention.add(nn.Dense(1, in_units=alignment_size,\n",
    "                                            flatten=False))\n",
    "\n",
    "            self.rnn = rnn.GRU(num_hiddens, num_layers, dropout=drop_prob,\n",
    "                               input_size=num_hiddens)\n",
    "            self.out = nn.Dense(num_outputs, in_units=num_hiddens,\n",
    "                                flatten=False)\n",
    "            self.rnn_concat_input = nn.Dense(\n",
    "                num_hiddens, in_units=num_hiddens + encoder_num_hiddens,\n",
    "                flatten=False)\n",
    "\n",
    "    def forward(self, cur_input, state, encoder_outputs):\n",
    "        # 当循环神经网络有多个隐藏层时，取最靠近输出层的单层隐藏状态。\n",
    "        single_layer_state = [state[0][-1].expand_dims(0)]\n",
    "        encoder_outputs = encoder_outputs.reshape((self.max_seq_len, -1,\n",
    "                                                   self.encoder_num_hiddens))\n",
    "        hidden_broadcast = nd.broadcast_axis(single_layer_state[0], axis=0,\n",
    "                                             size=self.max_seq_len)\n",
    "        encoder_outputs_and_hiddens = nd.concat(encoder_outputs,\n",
    "                                                hidden_broadcast, dim=2)\n",
    "        energy = self.attention(encoder_outputs_and_hiddens)\n",
    "        batch_attention = nd.softmax(energy, axis=0).transpose((1, 2, 0))\n",
    "        batch_encoder_outputs = encoder_outputs.swapaxes(0, 1)\n",
    "        decoder_context = nd.batch_dot(batch_attention, batch_encoder_outputs)\n",
    "        input_and_context = nd.concat(\n",
    "            nd.expand_dims(self.embedding(cur_input), axis=1),\n",
    "            decoder_context, dim=2)\n",
    "        concat_input = self.rnn_concat_input(input_and_context).reshape(\n",
    "            (1, -1, 0))\n",
    "        concat_input = self.dropout(concat_input)\n",
    "        state = [nd.broadcast_axis(single_layer_state[0], axis=0,\n",
    "                                   size=self.num_layers)]\n",
    "        output, state = self.rnn(concat_input, state)\n",
    "        output = self.dropout(output)\n",
    "        output = self.out(output).reshape((-3, -1))\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.rnn.begin_state(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义解码器初始状态\n",
    "\n",
    "为了初始化解码器的隐藏状态，我们通过一层全连接网络来变换编码器的输出隐藏状态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderInitState(nn.Block):\n",
    "    def __init__(self, encoder_num_hiddens, decoder_num_hiddens, **kwargs):\n",
    "        super(DecoderInitState, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.dense = nn.Dense(decoder_num_hiddens,\n",
    "                                  in_units=encoder_num_hiddens,\n",
    "                                  activation=\"tanh\", flatten=False)\n",
    "\n",
    "    def forward(self, encoder_state):\n",
    "        return [self.dense(encoder_state)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型并输出不定长序列\n",
    "\n",
    "我们定义 `translate` 函数应用训练好的模型，并通过贪婪搜索输出不定长的翻译文本序列。解码器的最初时间步输入来自“&lt;bos&gt;”符号。对于一个输出中的序列，当解码器在某一时间步搜索出“&lt;eos&gt;”符号时，即完成该输出序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(encoder, decoder, decoder_init_state, fr_ens, ctx, max_seq_len):\n",
    "    for fr_en in fr_ens:\n",
    "        print('[input] ', fr_en[0])\n",
    "        input_tokens = fr_en[0].split(' ') + [EOS]\n",
    "        # 添加 PAD 符号使每个序列等长（长度为 max_seq_len）。\n",
    "        while len(input_tokens) < max_seq_len:\n",
    "            input_tokens.append(PAD)\n",
    "        inputs = nd.array(input_vocab.to_indices(input_tokens), ctx=ctx)\n",
    "        encoder_state = encoder.begin_state(func=nd.zeros, batch_size=1,\n",
    "                                            ctx=ctx)\n",
    "        encoder_outputs, encoder_state = encoder(inputs.expand_dims(0),\n",
    "                                                 encoder_state)\n",
    "        encoder_outputs = encoder_outputs.flatten()\n",
    "        # 解码器的第一个输入为 BOS 符号。\n",
    "        decoder_input = nd.array([output_vocab.token_to_idx[BOS]], ctx=ctx)\n",
    "        decoder_state = decoder_init_state(encoder_state[0])\n",
    "        output_tokens = []\n",
    "\n",
    "        for _ in range(max_test_output_len):\n",
    "            decoder_output, decoder_state = decoder(\n",
    "                decoder_input, decoder_state, encoder_outputs)\n",
    "            pred_i = int(decoder_output.argmax(axis=1).asnumpy()[0])\n",
    "            # 当任一时间步搜索出 EOS 符号时，输出序列即完成。\n",
    "            if pred_i == output_vocab.token_to_idx[EOS]:\n",
    "                break\n",
    "            else:\n",
    "                output_tokens.append(output_vocab.idx_to_token[pred_i])\n",
    "            decoder_input = nd.array([pred_i], ctx=ctx)\n",
    "        print('[output]', ' '.join(output_tokens))\n",
    "        print('[expect]', fr_en[1], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面定义模型训练函数。为了初始化解码器的隐藏状态，我们通过一层全连接网络来变换编码器最早时间步的输出隐藏状态。解码器中，当前时间步的预测词将作为下一时间步的输入。其实，我们也可以使用样本输出序列在当前时间步的词作为下一时间步的输入。这叫作强制教学（teacher forcing）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "eos_id = output_vocab.token_to_idx[EOS]\n",
    "\n",
    "def train(encoder, decoder, decoder_init_state, max_seq_len, ctx,\n",
    "          eval_fr_ens):\n",
    "    encoder.initialize(init.Xavier(), ctx=ctx)\n",
    "    decoder.initialize(init.Xavier(), ctx=ctx)\n",
    "    decoder_init_state.initialize(init.Xavier(), ctx=ctx)\n",
    "    encoder_optimizer = gluon.Trainer(encoder.collect_params(), 'adam',\n",
    "                                      {'learning_rate': lr})\n",
    "    decoder_optimizer = gluon.Trainer(decoder.collect_params(), 'adam',\n",
    "                                      {'learning_rate': lr})\n",
    "    decoder_init_state_optimizer = gluon.Trainer(\n",
    "        decoder_init_state.collect_params(), 'adam', {'learning_rate': lr})\n",
    "\n",
    "    data_iter = gdata.DataLoader(dataset, batch_size, shuffle=True)\n",
    "    l_sum = 0\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        for x, y in data_iter:\n",
    "            cur_batch_size = x.shape[0]\n",
    "            with autograd.record():\n",
    "                l = nd.array([0], ctx=ctx)\n",
    "                valid_length = nd.array([0], ctx=ctx)\n",
    "                encoder_state = encoder.begin_state(\n",
    "                    func=nd.zeros, batch_size=cur_batch_size, ctx=ctx)\n",
    "                # encoder_outputs 包含了编码器在每个时间步的隐藏状态。\n",
    "                encoder_outputs, encoder_state = encoder(x, encoder_state)\n",
    "                encoder_outputs = encoder_outputs.flatten()\n",
    "                # 解码器的第一个输入为 BOS 符号。\n",
    "                decoder_input = nd.array(\n",
    "                    [output_vocab.token_to_idx[BOS]] * cur_batch_size,\n",
    "                    ctx=ctx)\n",
    "                mask = nd.ones(shape=(cur_batch_size,), ctx=ctx)\n",
    "                decoder_state = decoder_init_state(encoder_state[0])\n",
    "                for i in range(max_seq_len):\n",
    "                    decoder_output, decoder_state = decoder(\n",
    "                        decoder_input, decoder_state, encoder_outputs)\n",
    "                    # 解码器使用当前时间步的预测词作为下一时间步的输入。\n",
    "                    decoder_input = decoder_output.argmax(axis=1)\n",
    "                    valid_length = valid_length + mask.sum()\n",
    "                    l = l + (mask * loss(decoder_output, y[:, i])).sum()\n",
    "                    mask = mask * (y[:, i] != eos_id)\n",
    "                l = l / valid_length\n",
    "            l.backward()\n",
    "            encoder_optimizer.step(1)\n",
    "            decoder_optimizer.step(1)\n",
    "            decoder_init_state_optimizer.step(1)\n",
    "            l_sum += l.asscalar() / max_seq_len\n",
    "\n",
    "        if epoch % eval_interval == 0 or epoch == 1:\n",
    "            if epoch == 1:\n",
    "                print('epoch %d, loss %f, ' % (epoch, l_sum / len(data_iter)))\n",
    "            else:\n",
    "                print('epoch %d, loss %f, ' \n",
    "                      % (epoch, l_sum / eval_interval / len(data_iter)))\n",
    "            if epoch != 1:\n",
    "                l_sum = 0\n",
    "            translate(encoder, decoder, decoder_init_state, eval_fr_ens, ctx,\n",
    "                      max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下分别实例化编码器、解码器和解码器初始隐藏状态网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(len(input_vocab), encoder_embed_size, encoder_num_hiddens,\n",
    "                  encoder_num_layers, encoder_drop_prob)\n",
    "decoder = Decoder(decoder_num_hiddens, len(output_vocab),\n",
    "                  decoder_num_layers, max_seq_len, decoder_drop_prob,\n",
    "                  alignment_size, encoder_num_hiddens)\n",
    "decoder_init_state = DecoderInitState(encoder_num_hiddens,\n",
    "                                      decoder_num_hiddens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定简单的法语和英语序列，我们可以观察模型的训练结果。打印的结果中，input、output 和 expect 分别代表输入序列、输出序列和正确序列。我们可以比较 output 和 expect，观察输出序列是否符合预期。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.538751, \n",
      "[input]  elle est japonaise .\n",
      "[output] they are . . . . . . . . . . . . . . . . . .\n",
      "[expect] she is japanese . \n",
      "\n",
      "[input]  ils regardent .\n",
      "[output] they are . . . . . . . . . . . . . . . . . .\n",
      "[expect] they are watching . \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, loss 0.172950, \n",
      "[input]  elle est japonaise .\n",
      "[output] she is old .\n",
      "[expect] she is japanese . \n",
      "\n",
      "[input]  ils regardent .\n",
      "[output] they are watching .\n",
      "[expect] they are watching . \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20, loss 0.046971, \n",
      "[input]  elle est japonaise .\n",
      "[output] she is japanese .\n",
      "[expect] she is japanese . \n",
      "\n",
      "[input]  ils regardent .\n",
      "[output] they are watching .\n",
      "[expect] they are watching . \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30, loss 0.089269, \n",
      "[input]  elle est japonaise .\n",
      "[output] she is quiet .\n",
      "[expect] she is japanese . \n",
      "\n",
      "[input]  ils regardent .\n",
      "[output] they are watching .\n",
      "[expect] they are watching . \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40, loss 0.012273, \n",
      "[input]  elle est japonaise .\n",
      "[output] she is japanese .\n",
      "[expect] she is japanese . \n",
      "\n",
      "[input]  ils regardent .\n",
      "[output] they are watching .\n",
      "[expect] they are watching . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_fr_ens =[['elle est japonaise .', 'she is japanese .'],\n",
    "              ['ils regardent .', 'they are watching .']]\n",
    "train(encoder, decoder, decoder_init_state, max_seq_len, ctx, eval_fr_ens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了使模型能够翻译更复杂的句子，我们需要使用更大的训练数据集、调节超参数并增加训练时间。当然，我们还需要有验证数据集，并依据模型在它上面的表现调参。那么，该如何在验证数据集上评价模型的表现呢？这就需要评价翻译结果的指标。\n",
    "\n",
    "\n",
    "## 评价翻译结果\n",
    "\n",
    "2002 年，IBM 团队提出了一种评价翻译结果的指标，叫 BLEU（Bilingual Evaluation Understudy）[1]。\n",
    "\n",
    "设 $k$ 为我们希望评价的 $n$ 个连续词的最大长度，例如 $k=4$。设 $n$ 个连续词的精度为 $p_n$。它是模型预测序列与样本标签序列匹配 $n$ 个连续词的数量与模型预测序列中 $n$ 个连续词数量之比。举个例子，假设标签序列为 $ABCDEF$，预测序列为 $ABBCD$。那么 $p_1 = 4/5, p_2 = 3/4, p_3 = 1/3, p_4 = 0$。设 $len_{\\text{label}}$ 和 $len_{\\text{pred}}$ 分别为标签序列和模型预测序列的词数。那么，BLEU 的定义为\n",
    "\n",
    "$$ \\exp(\\min(0, 1 - \\frac{len_{\\text{label}}}{len_{\\text{pred}}})) \\prod_{i=1}^k p_n^{1/2^n}.$$\n",
    "\n",
    "需要注意的是，匹配较长连续词比匹配较短连续词更难。因此，一方面，匹配较长连续词应被赋予更大权重。而上式中 $p_n^{1/2^n}$ 的指数相当于权重。随着 $n$ 的提高，$n$ 个连续词的精度的权重随着 $1/2^n$ 的减小而增大。例如 $0.5^{1/2} \\approx 0.7, 0.5^{1/4} \\approx 0.84, 0.5^{1/8} \\approx 0.92, 0.5^{1/16} \\approx 0.96$。另一方面，模型预测较短序列往往会得到较高的 $n$ 个连续词的精度。因此，上式中连乘项前面的系数是为了惩罚较短的输出。举个例子，当 $k=2$ 时，假设标签序列为 $ABCDEF$，而预测序列为 $AB$。虽然 $p_1 = p_2 = 1$，但惩罚系数 $\\exp(1-6/2) \\approx 0.14$，因此 BLEU 也接近 0.14。当预测序列和标签序列完全一致时，BLEU 为 1。\n",
    "\n",
    "## 小结\n",
    "\n",
    "* 我们可以将编码器—解码器和注意力机制应用于机器翻译中。\n",
    "* BLEU 可以用来评价翻译结果。\n",
    "\n",
    "\n",
    "## 练习\n",
    "\n",
    "* 试着使用更大的翻译数据集来训练模型，例如 WMT [2] 和 Tatoeba Project [3]。\n",
    "* 在解码器中使用强制教学，观察实现现象。\n",
    "\n",
    "## 扫码直达 [ 讨论区 ](https://discuss.gluon.ai/t/topic/4689)\n",
    "\n",
    "![](../img/qr_machine-translation.svg)\n",
    "\n",
    "## 参考文献\n",
    "\n",
    "[1] Papineni, K., Roukos, S., Ward, T., & Zhu, W. J. (2002, July). BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics (pp. 311-318). Association for Computational Linguistics.\n",
    "\n",
    "[2] WMT. http://www.statmt.org/wmt14/translation-task.html\n",
    "\n",
    "[3] Tatoeba Project. http://www.manythings.org/anki/"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}