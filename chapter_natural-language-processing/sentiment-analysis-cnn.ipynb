{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本分类：情感分析\n",
    "\n",
    "在之前的章节中介绍了卷积神经网络用于计算机视觉领域。\n",
    "在本节将介绍如何将卷积神经网络应用于自然语言处理领域。以及参考 textCNN 模型使用 Gluon 创建一个卷积神经网络用于文本情感分类。\n",
    "\n",
    "## 模型设计\n",
    "\n",
    "卷积神经网络在自然语言处理上，可以类比到图像任务上，即把一个文本用二维图像的方式来表达。每个文本是一个矩阵，将文本中每个词的词向量按顺序纵向排列，即这个矩阵的每一行分别是一个词向量。\n",
    "\n",
    "在卷积层中，使用不同的卷积核获取不同窗口大小内词的关系；而与计算机视觉中的二维卷积不同的是，自然语言处理任务中一般用的是一维卷积，即卷积核的宽度是词嵌入的维度。因为我们需要获取的是不同窗口内的词所带来的信息。然后，我们应用一个最大池化层，这里采用的是 `Max-over-time pooling`，即对一个 feature map 选取一个最大值保留，这个最大值可以理解为是这个 feature map 最重要的特征。将这些取到的最大值连结成一个向量。而由于只取最大值，在做 padding 时补 0，并不会影响结果。\n",
    "\n",
    "最后，我们将连结得到的向量通过全连接层变换为输出。我们在全连接层前加一个 Dropout 层，用于减轻过拟合。\n",
    "\n",
    "![](../img/textcnn.svg)\n",
    "\n",
    "\n",
    "我们来描述一下这个过程：\n",
    "1. 我们假设有一个文本，长度 n 为 11 ，词嵌入维度为 7 。此时词嵌入矩阵维度为（11， 7）。\n",
    "2. 设有三组卷积核，卷积宽度 f 分别是 2、3、4，卷积核的数目分别为 4、4、5 。卷积后得到的矩阵维度分别是，（10，4）、（9，4）、（8，5）。即（n-f+1，nums_channels）\n",
    "3. 再进行 Max-over-time pooling，得到的矩阵维度分别是 (4，1)、(4，1)、(5，1)。\n",
    "4. 压平上述三个矩阵，并连结，得到一个（4+4+5）维度的向量\n",
    "5. 再通过一个全连接层降低维度。\n",
    "\n",
    "在实验开始前，导入所需的包或模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import collections\n",
    "import gluonbook as gb\n",
    "import mxnet as mx\n",
    "from mxnet import autograd, gluon, init, metric, nd\n",
    "from mxnet.contrib import text\n",
    "from mxnet.gluon import loss as gloss, nn, rnn\n",
    "import os\n",
    "import random\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取 IMDb 数据集\n",
    "\n",
    "我们使用 Stanford's Large Movie Review Dataset 作为情感分析的数据集 [1]。它的下载地址是\n",
    "\n",
    "> http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz 。\n",
    "\n",
    "这个数据集分为训练和测试用的两个数据集，分别有 25,000 条从 IMDb 下载的关于电影的评论。在每个数据集中，标签为“正面”（1）和“负面”（0）的评论数量相等。将下载好的数据解压并存放在路径“../data/aclImdb”。\n",
    "\n",
    "为方便快速上手，我们提供了上述数据集的小规模采样，并存放在路径“../data/aclImdb_tiny.zip”。如果你将使用上述的 IMDb 完整数据集，还需要把下面 `demo` 变量改为 `False`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "# 如果使用下载的 IMDb 的完整数据集，把下面改为 False。\n",
    "demo = True\n",
    "if demo:\n",
    "    with zipfile.ZipFile('../data/aclImdb_tiny.zip', 'r') as zin:\n",
    "        zin.extractall('../data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面，读取训练和测试数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "def readIMDB(dir_url, seg='train'):\n",
    "    pos_or_neg = ['pos', 'neg']\n",
    "    data = []\n",
    "    for label in pos_or_neg:\n",
    "        files = os.listdir(os.path.join('../data/',dir_url, seg, label))\n",
    "        for file in files:\n",
    "            with open(os.path.join('../data/',dir_url, seg, label, file), 'r', encoding='utf8') as rf:\n",
    "                review = rf.read().replace('\\n', '')\n",
    "                if label == 'pos':\n",
    "                    data.append([review, 1])\n",
    "                elif label == 'neg':\n",
    "                    data.append([review, 0])\n",
    "    return data\n",
    "\n",
    "if demo:\n",
    "    train_data = readIMDB('aclImdb_tiny', 'train')\n",
    "    test_data = readIMDB('aclImdb_tiny', 'test')\n",
    "else:\n",
    "    train_data = readIMDB('aclImdb', 'train')\n",
    "    test_data = readIMDB('aclImdb', 'test')\n",
    "\n",
    "random.shuffle(train_data)\n",
    "random.shuffle(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分词\n",
    "\n",
    "接下来我们对每条评论做分词，从而得到分好词的评论。这里使用最简单的方法：基于空格进行分词。我们将在本节练习中探究其他的分词方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return [tok.lower() for tok in text.split(' ')]\n",
    "\n",
    "train_tokenized = []\n",
    "for review, score in train_data:\n",
    "    train_tokenized.append(tokenizer(review))\n",
    "test_tokenized = []\n",
    "for review, score in test_data:\n",
    "    test_tokenized.append(tokenizer(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建词典\n",
    "\n",
    "现在，我们可以根据分好词的训练数据集来创建词典了。这里我们设置了特殊符号“&lt;unk&gt;”（unknown）。它将表示一切不存在于训练数据集词典中的词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "token_counter = collections.Counter()\n",
    "def count_token(train_tokenized):\n",
    "    for sample in train_tokenized:\n",
    "        for token in sample:\n",
    "            if token not in token_counter:\n",
    "                token_counter[token] = 1\n",
    "            else:\n",
    "                token_counter[token] += 1\n",
    "\n",
    "count_token(train_tokenized)\n",
    "vocab = text.vocab.Vocabulary(token_counter, unknown_token='<unk>',\n",
    "                              reserved_tokens=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理数据\n",
    "\n",
    "下面，我们继续对数据进行预处理。每个不定长的评论将被特殊符号 `PAD` 补成长度为 `maxlen` 的序列，并用 NDArray 表示。在这里由于模型使用了最大池化层，只取卷积后最大的一个值，所以补 0 不会对结果产生影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [],
   "source": [
    "def encode_samples(tokenized_samples, vocab):\n",
    "    features = []\n",
    "    for sample in tokenized_samples:\n",
    "        feature = []\n",
    "        for token in sample:\n",
    "            if token in vocab.token_to_idx:\n",
    "                feature.append(vocab.token_to_idx[token])\n",
    "            else:\n",
    "                feature.append(0)\n",
    "        features.append(feature)         \n",
    "    return features\n",
    "\n",
    "def pad_samples(features, maxlen=500, PAD=0):\n",
    "    padded_features = []\n",
    "    for feature in features:\n",
    "        if len(feature) > maxlen:\n",
    "            padded_feature = feature[:maxlen]\n",
    "        else:\n",
    "            padded_feature = feature\n",
    "            # 添加 PAD 符号使每个序列等长（长度为 maxlen ）。\n",
    "            while len(padded_feature) < maxlen:\n",
    "                padded_feature.append(PAD)\n",
    "        padded_features.append(padded_feature)\n",
    "    return padded_features\n",
    "\n",
    "ctx = gb.try_gpu()\n",
    "train_features = encode_samples(train_tokenized, vocab)\n",
    "test_features = encode_samples(test_tokenized, vocab)\n",
    "train_features = nd.array(pad_samples(train_features, 1000, 0), ctx=ctx)\n",
    "test_features = nd.array(pad_samples(test_features, 1000, 0), ctx=ctx)\n",
    "train_labels = nd.array([score for _, score in train_data], ctx=ctx)\n",
    "test_labels = nd.array([score for _, score in test_data], ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载预训练的词向量\n",
    "\n",
    "这里，我们为词典 `vocab` 中的每个词加载 GloVe 词向量（每个词向量长度为 100）。稍后，我们将用这些词向量作为评论中每个词的特征向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [],
   "source": [
    "glove_embedding = text.embedding.create(\n",
    "    'glove', pretrained_file_name='glove.6B.100d.txt', vocabulary=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一维卷积\n",
    "\n",
    "一维卷积即将一个一维核（kernel）数组作用在一个一维输入数据上来计算一个一维数组输出。下图演示了如何对一个长度为 7 的输入 X 作用宽为 2 的核 K 来计算输出 Y。\n",
    "\n",
    "![](../img/Conv1D.svg)\n",
    "\n",
    "可以看到输出 Y 是一个 6 维的向量，且第一个元素是由 X 的最左侧宽为 2 的子数组与核数组按元素相乘后再相加得来。即 Y[0] = (X[0:2] * K).sum()。卷积后输出的数据维度仍遵循 n-f+1，即 7-2+1=6\n",
    "\n",
    "一维卷积常用于序列模型，比如自然语言处理领域中。\n",
    "\n",
    "下面我们将上述过程实现在 corr1d 函数里，它接受 X 和 K，输出 Y。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import gluonbook as gb\n",
    "from mxnet import autograd, nd\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "def corr1d(X, K):\n",
    "    w = K.shape[0]\n",
    "    Y = nd.zeros((X.shape[0] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        Y[i] = (X[i : i + w] * K).sum()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造上图中的数据来测试实现的正确性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[  2.   5.   8.  11.  14.  17.]\n",
       "<NDArray 6 @cpu(0)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = nd.array([0 ,1 ,2, 3, 4 ,5 ,6])\n",
    "K = nd.array([1 ,2])\n",
    "corr1d(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一维卷积多通道输入的卷积运算与二维卷积的多通道运算类似。将每个单通道与对应的 filter 进行卷积运算求和，然后再将多个通道的和相加，得到输出的一个数值。\n",
    "\n",
    "![](../img/Conv1D-channel.svg)\n",
    "\n",
    "解释上图，假设存在三个通道 $ c_0, c_1, c_2 $，存在一组卷积核 $ k_0, k_1, k_2 $\n",
    "\n",
    "$$ y(i)=\\sum_m c_0(i-m)k_0(m) + \\sum_m c_1(i-m)k_1(m) + \\sum_m c_2(i-m)k_2(m) \\\\\n",
    "=\\sum_m \\sum_{n\\in\\{0, 1, 2\\}} c_n(i-m)k_n(m) $$\n",
    "\n",
    "![](../img/Conv1D-flatten.svg)\n",
    "\n",
    "我们将 $ c_0, c_1, c_2 $ 三个向量连结成矩阵 C，将 $ k_0, k_1, k_2 $ 连结成矩阵 K\n",
    "\n",
    "$$ y(i)=\\sum_m \\sum_{n\\in\\{0, 1, 2\\}} C(i-m,j-n)K(m,n) $$\n",
    "\n",
    "上式与二维卷积的定义等价。\n",
    "\n",
    "故：多通道一维卷积计算可以等价于单通道二维卷积计算。\n",
    "\n",
    "类比到图像上，我们可以用一个三维的向量（R, G, B）来表达一个像素点。在做卷积时将 R、G、B 作为三个通道来进行运算。\n",
    "\n",
    "在文本任务上，我们可以用一个 k 维的向量来表达一个词，即词向量。这个 k 叫做嵌入层维度 embed_size。同样的，在做卷积时也将这 k 维作为 k 个通道来进行计算。\n",
    "\n",
    "所以，对于自然语言处理任务而言，输入的通道数等于嵌入层维度 embed_size。\n",
    "\n",
    "## 定义模型\n",
    "\n",
    "下面我们根据模型设计里的描述定义情感分类模型。其中的 `Embedding` 实例即嵌入层，在实验中，我们使用了两个嵌入层。`Conv1D` 实例即为卷积层，`GlobalMaxPool1D` 实例为池化层，卷积层和池化层用于抽取文本中重要的特征。`Dense` 实例即生成分类结果的输出层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    }
   },
   "outputs": [],
   "source": [
    "class TextCNN(nn.Block):\n",
    "    def __init__(self, vocab, embedding_size, ngram_kernel_sizes, nums_channels, **kwargs):\n",
    "        super(TextCNN, self).__init__(**kwargs)\n",
    "        self.ngram_kernel_sizes = ngram_kernel_sizes\n",
    "        self.nums_channels = nums_channels\n",
    "        self.embedding_static = nn.Embedding(len(vocab), embedding_size)\n",
    "        self.embedding_non_static = nn.Embedding(len(vocab), embedding_size)\n",
    "        for i in range(len(ngram_kernel_sizes)):\n",
    "            conv = nn.Conv1D(nums_channels[i],\n",
    "                kernel_size=ngram_kernel_sizes[i],\n",
    "                strides=1,\n",
    "                activation='relu')\n",
    "            pool = nn.GlobalMaxPool1D()\n",
    "            setattr(self, f'conv_{i}', conv)  # 将 self.conv_{i} 置为第 i 个 conv\n",
    "            setattr(self, f'pool_{i}', pool)  # 将 self.pool_{i} 置为第 i 个 pool\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.decoder = nn.Dense(num_outputs)\n",
    "    def forward(self, inputs):\n",
    "        #inputs 的维度为 ( 句子长度 , batch_size) \n",
    "        \n",
    "        #embeddings_static 的维度为（句子长度 , batch_size, embed_size） \n",
    "        embeddings_static = self.embedding_static(inputs)\n",
    "        \n",
    "        #Conv1D 要求的输入是（batch_size, in_channels, width），故需要做 transpose((1,2,0))\n",
    "        embeddings_static = embeddings_static.transpose((1,2,0))  \n",
    "        \n",
    "        #embeddings_non_static 的维度同上\n",
    "        embeddings_non_static = self.embedding_non_static(inputs).transpose((1,2,0))\n",
    "        \n",
    "        # 按照模型设计，每个卷积核都应用于两个嵌入层，此时卷积核为的核数组。将卷积核在多个嵌入层的运算结果求和，即得到一次卷积结果。\n",
    "        # 这等价于直接连结这两个嵌入层，再将卷积核变成（ngram_kernel_sizes, nums_channels * 2 ），所得结果相同。\n",
    "        #dim = 1 的意思是从 in_channels 这个维度进行连结。连结后的维度是（batch_size, in_channels*2, width）\n",
    "        embeddings = nd.concat(embeddings_static, embeddings_non_static, dim=1)\n",
    "        \n",
    "        # 对于卷积核 [i]，在池化之后会形成一个（nums_channels[i],1）的矩阵，需要使用 flatten 压平成 nums_channels[i] 维的向量\n",
    "        encoding = [\n",
    "            nd.flatten(self.get_pool(i)(self.get_conv(i)(embeddings)))\n",
    "            for i in range(len(self.ngram_kernel_sizes))]\n",
    "        \n",
    "        # 在此之前， encoding 有多个元素，每个元素维度是（batch_size, nums_channels[i]）。\n",
    "        # 需要将它连结成一个维度为（batch_size, nums_channels 的和）的矩阵\n",
    "        encoding = nd.concat(*encoding, dim=1)\n",
    "        \n",
    "        outputs = self.decoder(self.dropout(encoding))\n",
    "        return outputs\n",
    "    # 调用 self.get_conv(i)，即返回 self.conv_{i}\n",
    "    def get_conv(self, i):\n",
    "        return getattr(self, f'conv_{i}')\n",
    "    # 调用 self.get_pool(i)，即返回 self.pool_{i}\n",
    "    def get_pool(self, i):\n",
    "        return getattr(self, f'pool_{i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用在更大规模语料上预训练的词向量作为每个词的特征向量。本实验有两个嵌入层，其中嵌入层 `Embedding_non_static` 的词向量可以在训练过程中被更新，另一个嵌入层 `Embedding_static` 的词向量在训练过程中不能被更新。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    }
   },
   "outputs": [],
   "source": [
    "num_outputs = 2\n",
    "lr = 0.002\n",
    "num_epochs = 1\n",
    "batch_size = 10\n",
    "embed_size = 100\n",
    "ngram_kernel_sizes = [3, 4, 5]\n",
    "nums_channels = [100, 100, 100]\n",
    "\n",
    "net = TextCNN(vocab, embed_size, ngram_kernel_sizes, nums_channels)\n",
    "net.initialize(init.Xavier(), ctx=ctx)\n",
    "# 设置两个 embedding 层的 weight 为预训练的词向量。\n",
    "net.embedding_static.weight.set_data(glove_embedding.idx_to_vec.as_in_context(ctx))\n",
    "net.embedding_non_static.weight.set_data(glove_embedding.idx_to_vec.as_in_context(ctx))\n",
    "# 训练中不更新 embedding_static 的词向量（net.embedding 中的模型参数）。\n",
    "net.embedding_static.collect_params().setattr('grad_req', 'null')\n",
    "trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr})\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练并评价模型\n",
    "\n",
    "在实验中，我们使用准确率作为评价模型的指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    }
   },
   "outputs": [],
   "source": [
    "def eval_model(features, labels):\n",
    "    l_sum = 0\n",
    "    l_n = 0\n",
    "    accuracy = metric.Accuracy()\n",
    "    for i in range(features.shape[0] // batch_size ):\n",
    "        X = features[i*batch_size : (i + 1) * batch_size ].as_in_context(ctx).T\n",
    "        y = labels[i*batch_size :(i + 1) * batch_size ].as_in_context(ctx).T\n",
    "        output = net(X)\n",
    "        l = loss(output, y)\n",
    "        l_sum += l.sum().asscalar()\n",
    "        l_n += l.size\n",
    "        accuracy.update(preds=nd.argmax(output, axis=1), labels=y)\n",
    "    return l_sum / l_n, accuracy.get()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面开始训练模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, train loss 1.478550, acc 0.50; test loss 2.347687, acc 0.50\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs + 1):\n",
    "    for i in range(train_features.shape[0] // batch_size):\n",
    "        X = train_features[i*batch_size : (i+1)*batch_size].as_in_context(\n",
    "            ctx).T\n",
    "        y = train_labels[i*batch_size : (i+1)*batch_size].as_in_context(\n",
    "            ctx).T\n",
    "        with autograd.record():\n",
    "            l = loss(net(X), y)\n",
    "        l.backward()\n",
    "        trainer.step(batch_size)\n",
    "    train_loss, train_acc = eval_model(train_features, train_labels)\n",
    "    test_loss, test_acc = eval_model(test_features, test_labels)\n",
    "    print('epoch %d, train loss %.6f, acc %.2f; test loss %.6f, acc %.2f' \n",
    "          % (epoch, train_loss, train_acc, test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 我们可以使用一维卷积来处理时序序列任务，如自然语言处理。\n",
    "\n",
    "* 多通道一维卷积运算可以等价于单通道二维卷积计算。\n",
    "\n",
    "\n",
    "## 练习\n",
    "\n",
    "* 使用 IMDb 完整数据集。你的模型能在训练和测试数据集上得到怎样的准确率？通过调节超参数，你能进一步提升分类准确率吗？\n",
    "\n",
    "* 使用更大的预训练词向量，例如 300 维的 GloVe 词向量，能否提升分类准确率？\n",
    "\n",
    "* 使用 spacy 分词工具，能否提升分类准确率？。你需要安装 spacy：`pip install spacy`，并且安装英文包：`python -m spacy download en`。在代码中，先导入 spacy：`import spacy`。然后加载 spacy 英文包：`spacy_en = spacy.load('en')`。最后定义函数：`def tokenizer(text): return [tok.text for tok in spacy_en.tokenizer(text)]` 替换原来的基于空格分词的 `tokenizer` 函数。需要注意的是，GloVe 的词向量对于名词词组的存储方式是用“-”连接各个单词，例如词组“new york”在 GloVe 中的表示为“new-york”。而使用 spacy 分词之后“new york”的存储可能是“new york”。\n",
    "\n",
    "* 通过上面三种方法，你能使模型在测试集上的准确率提高到 0.86 以上吗？\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 扫码直达 [ 讨论区 ](https://discuss.gluon.ai/t/topic/6155)\n",
    "\n",
    "\n",
    "![](../img/qr_sentiment-analysis.svg)\n",
    "\n",
    "\n",
    "## 参考文献\n",
    "\n",
    "[1] Kim, Y. (2014). Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}