{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练词嵌入模型\n",
    "\n",
    "#TODO(@astonzhang) Need edit.\n",
    "\n",
    "我们在 [“词向量：word2vec”](./word2vec.md)introduced the word2vec word embedding model. In this notebook we will show how to train a word2vec model with Gluon. We will introduce training the model with the skip-gram objective and negative sampling. Besides mxnet Gluon we will only use standard Python language features but note that specific  toolkits for Natural Language Processing, such as the Gluon-NLP toolkit exist.\n",
    "\n",
    "首先导入实验所需的包或模块，并抽取数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import nd, gluon\n",
    "from mxnet.gluon import data as gdata, loss as gloss, nn\n",
    "import collections\n",
    "import functools\n",
    "import itertools\n",
    "import math\n",
    "import mxnet as mx\n",
    "import operator\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We then load a corpus for training the word embedding model. Like for training the language model in [“循环神经网络——使用 Gluon”](../chapter_recurrent-neural-networks/rnn-gluon.md), we use the Penn Tree Bank（PTB）[1]。它包括训练集、验证集和测试集 。We directly split the datasets into sentences and tokens, considering newlines as paragraph delimeters and whitespace as token delimiter. We print the first five words of the first three sentences of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aer', 'banknote', 'berlitz', 'calloway', 'centrust', '...']\n",
      "['pierre', '<unk>', 'N', 'years', 'old', '...']\n",
      "['mr.', '<unk>', 'is', 'chairman', 'of', '...']\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('../data/ptb.zip', 'r') as zin:\n",
    "    zin.extractall('../data/')\n",
    "    \n",
    "with open('../data/ptb/ptb.train.txt', 'r') as f:\n",
    "    dataset = f.readlines()\n",
    "    dataset = [sentence.split() for sentence in dataset]\n",
    "\n",
    "for sentence in dataset[:3]:\n",
    "    print(sentence[:5] + ['...'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立词语索引\n",
    "\n",
    "下面定义了 `Dictionary` 类来映射词语和整数索引。We first count all tokens in the dataset and assign integer indices to all tokens that occur more than five times in the corpus. We also construct the reverse mapping token to integer index `token_to_idx` and finally replace all tokens in the dataset with their respective indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "min_token_occurence = 5\n",
    "\n",
    "counter = collections.Counter(itertools.chain.from_iterable(dataset))\n",
    "idx_to_token = list(token_count[0] for token_count in \n",
    "                    filter(lambda token_count: token_count[1] >= min_token_occurence,\n",
    "                           counter.items()))\n",
    "token_to_idx = {token: idx for idx, token in enumerate(idx_to_token)}\n",
    "\n",
    "coded_dataset = [[token_to_idx[token] for token in sentence if token in token_to_idx] for sentence in dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset subsampling\n",
    "\n",
    "One important trick applied when training word2vec is to subsample the dataset\n",
    "according to the token frequencies. [2] proposes to discards individual\n",
    "occurences of words from the dataset with probability $$ P(w_i) = 1 -\n",
    "\\sqrt{\\frac{t}{f(w_i)}}$$ where $f(w_i)$ is the frequency with which a word is\n",
    "observed in a dataset and $t$ is a subsampling constant typically chosen around\n",
    "$10^{-5}$. We are using a very small dataset here and found results in this case to be better with\n",
    "$10^{-4}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_counts = [counter[w] for w in idx_to_token]\n",
    "frequent_tokens_subsampling_constant = 1e-4\n",
    "sum_counts =  sum(idx_to_counts)\n",
    "idx_to_pdiscard = [1 - math.sqrt(frequent_tokens_subsampling_constant / (count / sum_counts))\n",
    "                   for count in idx_to_counts]\n",
    "\n",
    "pruned_dataset = [[t for t in s if random.uniform(0, 1) > idx_to_pdiscard[t]] for s in coded_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation of data\n",
    "\n",
    "The skip-gram objective with negative sampling is based on sampled center,\n",
    "context and negative data. 在跳字模型中，我们用一个词来预测它在文本序列周围的词。\n",
    "举个例子，假设文本序列是“the”、“man”、“hit”、“his”和“son”。跳字模型所关心的是，\n",
    "给定“hit”生成邻近词“the”、“man”、“his”和“son”的条件概率。在这个例子中，“hit”叫中\n",
    "心词，“the”、“man”、“his”和“son”叫背景词。由于“hit”只生成与它距离不超过 2 的背景词，\n",
    "该时间窗口的大小为 2。\n",
    "\n",
    "In general it is common to chose a maximum context size of for example 5 and to\n",
    "uniformly sample a smaller context size from the interval [1, 5] independently\n",
    "for each center word. So if we sample a random reduced context size of 1 在这个\n",
    "例子中，只“man”和“his”叫背景词。\n",
    "\n",
    "To train our Word2Vec model with batches of data we need to make sure that all\n",
    "elements of a batch have the same shape, ie. the same context length. However\n",
    "due to sampling a random reduced context size and as it is not guaranteed that a\n",
    "sufficient number of words precedes or follows a given center word (as it may be\n",
    "at the beginning or end of a sentence) the number of context words for a given\n",
    "center word is not constant. Consequently we pad the context arrays and\n",
    "introduce a mask that tells the model which of the words in the context array\n",
    "are real context words and which are just padding.\n",
    "\n",
    "For big datasets it is important to sample center and context words in a\n",
    "streaming manner. Here for simplicity and as we use a small dataset we transform\n",
    "the whole dataset at once into center words with respective contexts so that\n",
    "during training we only need to iterate over the pre-computed arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "def get_center_context_arrays(coded_sentences, window_size):\n",
    "    centers = []\n",
    "    contexts = []\n",
    "    masks = []\n",
    "    for sentence in coded_sentences:\n",
    "        # We need at least  2  words  to form a source, context pair\n",
    "        if len(sentence) < 2:\n",
    "            continue\n",
    "        centers += sentence\n",
    "        context = [get_one_context(sentence, i, window_size)\n",
    "                   for i in range(len(sentence))]\n",
    "        contexts += context\n",
    "    return centers, contexts\n",
    "\n",
    "\n",
    "def get_one_context(sentence, word_index, window_size):\n",
    "    # A random reduced window size is drawn.\n",
    "    random_window_size = random.randint(1, window_size)\n",
    "\n",
    "    start_idx = max(0, word_index - random_window_size)\n",
    "    # First index outside of the window\n",
    "    end_idx = min(len(sentence), word_index + random_window_size + 1)\n",
    "\n",
    "    context = []\n",
    "    # Get contexts left of center word\n",
    "    if start_idx != word_index:\n",
    "        context += sentence[start_idx:word_index]\n",
    "    # Get contexts right of center word\n",
    "    if word_index + 1 != end_idx: \n",
    "        context += sentence[word_index + 1:end_idx]\n",
    "    return context\n",
    "\n",
    "window_size = 5\n",
    "all_centers, all_contexts = get_center_context_arrays(pruned_dataset, window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 负采样\n",
    "\n",
    "Remember that the loss function for negative sampling is defined as\n",
    "\n",
    "$$ - \\text{log} \\mathbb{P} (w_o \\mid w_c) = -\\text{log} \\frac{1}{1+\\text{exp}(-\\mathbf{u}_o^\\top \\mathbf{v}_c)}  - \\sum_{k=1, w_k \\sim \\mathbb{P}(w)}^K \\text{log} \\frac{1}{1+\\text{exp}(\\mathbf{u}_{i_k}^\\top \\mathbf{v}_c)}. $$\n",
    "\n",
    "Consequently for training the model we need to sample negatives from the unigram\n",
    "token frequency distribution. The distribution is typically distorted by raising it elementwise to the  \n",
    "power 0.75.\n",
    "\n",
    "Note that while sampling from the unigram distribution is simple, we may\n",
    "accidentally sample a word as a negative that is actually in the current context\n",
    "of the center word. To improve training stability, we mask such accidental hits.\n",
    "\n",
    "Here we directly sample negatives for every context precomputed before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [],
   "source": [
    "def get_negatives(shape, true_samples, negatives_weights):\n",
    "    population = list(range(len(negatives_weights)))\n",
    "    k = functools.reduce(operator.mul, shape)\n",
    "    assert len(shape) == 2\n",
    "    assert len(true_samples) == shape[0]\n",
    "    \n",
    "    negatives = random.choices(population, weights=negatives_weights, k=k)\n",
    "    negatives = [negatives[i:i+shape[1]] for i in range(0, k, shape[1])]\n",
    "    negatives = [\n",
    "        [negative for negative in negatives_batch\n",
    "        if negative not in true_samples[i]]\n",
    "        for i, negatives_batch in enumerate(negatives)\n",
    "    ]\n",
    "    return negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [],
   "source": [
    "# This may take around 20 seconds\n",
    "num_negatives = 5\n",
    "negatives_weights = [counter[w]**0.75 for w in idx_to_token]\n",
    "negatives_shape = (len(all_contexts), window_size * 2 * num_negatives)\n",
    "all_negatives = get_negatives(negatives_shape, all_contexts, negatives_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "First we define a helper function `get_knn` to obtain the k closest words to for\n",
    "a given word according to our trained word embedding model to evaluate if it\n",
    "learned successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    }
   },
   "outputs": [],
   "source": [
    "def norm_vecs_by_row(x):\n",
    "    # 分母中添加的 1e-10 是为了数值稳定性。\n",
    "    return x / (nd.sum(x * x, axis=1) + 1e-10).sqrt().reshape((-1, 1))\n",
    "\n",
    "def get_knn(token_to_idx, idx_to_token, embedding, k, word):\n",
    "    word_vec = embedding(nd.array([token_to_idx[word]], ctx=context)).reshape((-1, 1))\n",
    "    vocab_vecs = norm_vecs_by_row(embedding.weight.data())\n",
    "    dot_prod = nd.dot(vocab_vecs, word_vec)\n",
    "    indices = nd.topk(dot_prod.reshape((len(idx_to_token), )), k=k+1, ret_typ='indices')\n",
    "    indices = [int(i.asscalar()) for i in indices]\n",
    "    # Remove unknown and input tokens.\n",
    "    result = [idx_to_token[i] for i in indices[1:]]\n",
    "    print('Closest tokens to \"%s\": %s' % (word, \", \".join(result)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define the model and initialize it randomly. Here we denote the model containing the weights $\\mathbf{v}$ as `embedding` and respectively the model for $\\mathbf{u}$ as `embedding_out`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "15"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest tokens to \"president\": hoping, tailored, upheld, manila, borough\n"
     ]
    }
   ],
   "source": [
    "context = mx.gpu(0)\n",
    "# context = mx.cpu()\n",
    "\n",
    "embedding_size = 300\n",
    "embedding = nn.Embedding(input_dim=len(idx_to_token), output_dim=embedding_size)\n",
    "embedding_out = nn.Embedding(input_dim=len(idx_to_token), output_dim=embedding_size)\n",
    "\n",
    "embedding.initialize(ctx=context)\n",
    "embedding_out.initialize(ctx=context)\n",
    "embedding.hybridize()\n",
    "embedding_out.hybridize()\n",
    "\n",
    "params = list(embedding.collect_params().values()) + list(embedding_out.collect_params().values())\n",
    "trainer = gluon.Trainer(params, 'adagrad', dict(learning_rate=0.1))\n",
    "\n",
    "example_token = 'president'\n",
    "knn = get_knn(token_to_idx, idx_to_token, embedding, 5, example_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gluon `SigmoidBinaryCrossEntropyLoss` corresponds to the loss function introduced above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "16"
    }
   },
   "outputs": [],
   "source": [
    "loss = gloss.SigmoidBinaryCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we train the word2vec model. We first shuffle our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(gdata.SimpleDataset):\n",
    "    def __init__(self, centers, contexts, negatives):\n",
    "        data = list(zip(centers, contexts, negatives))\n",
    "        super().__init__(data)\n",
    "\n",
    "def batchify_fn(data):\n",
    "    # data is a list with batch_size elements\n",
    "    # each element is of form (center, context, negative)\n",
    "    centers, contexts, negatives = zip(*data)\n",
    "    batch_size = len(centers)  # == len(contexts) == len(negatives)\n",
    "    \n",
    "    # contexts and negatives are of variable length\n",
    "    # we pad them to a fixed length and introduce a mask\n",
    "    length = max(len(c) + len(n) for c, n in zip(contexts, negatives))\n",
    "    contexts_negatives = []\n",
    "    masks = []\n",
    "    labels = []\n",
    "    for i, (context, negative) in enumerate(zip(contexts, negatives)):\n",
    "        len_context_negative = len(context) + len(negative)\n",
    "        context_negative = context + negative + [0] * (length - len_context_negative)\n",
    "        mask = [1] * len_context_negative + [0] * (length - len_context_negative)\n",
    "        label = [1] * len(context) + [0] * (length - len(context))\n",
    "        contexts_negatives.append(context_negative)\n",
    "        masks.append(mask)\n",
    "        labels.append(label)\n",
    "        \n",
    "    centers_nd = nd.array(centers).reshape((batch_size, 1))\n",
    "    contexts_negatives_nd = nd.array(contexts_negatives)\n",
    "    masks_nd = nd.array(masks)\n",
    "    labels_nd = nd.array(labels)\n",
    "    return centers_nd, contexts_negatives_nd, masks_nd, labels_nd\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "data = Dataset(all_centers, all_contexts, all_negatives)\n",
    "batches = gdata.DataLoader(data, batch_size=batch_size,\n",
    "                                shuffle=True, batchify_fn=batchify_fn,\n",
    "                                num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "17"
    }
   },
   "outputs": [],
   "source": [
    "def train_embedding(num_epochs=3, eval_period=100):\n",
    "    batch_id = 0\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        start_time = time.time()\n",
    "        train_l_sum = 0\n",
    "        for batch_i, (center, context_and_negative, mask, label) in enumerate(batches):\n",
    "            center = center.as_in_context(context)\n",
    "            context_and_negative = context_and_negative.as_in_context(context)\n",
    "            mask = mask.as_in_context(context)\n",
    "            label = label.as_in_context(context)\n",
    "            with mx.autograd.record():\n",
    "                # 1. Compute the embedding of the center words\n",
    "                emb_in = embedding(center)\n",
    "                # 2. Compute the context embedding\n",
    "                emb_out = embedding_out(context_and_negative) * mask.expand_dims(-1)\n",
    "                # 3. Compute the prediction\n",
    "                pred = nd.batch_dot(emb_in, emb_out.swapaxes(1, 2))\n",
    "                # 4. Compute the Loss function (SigmoidBinaryCrossEntropyLoss)\n",
    "                l = loss(pred, label)\n",
    "            # Compute the gradient\n",
    "            l.backward()\n",
    "            # Update the parameters\n",
    "            trainer.step(batch_size=1)\n",
    "            train_l_sum += l.mean()\n",
    "            if batch_i % eval_period == 0 and batch_i != 0 :\n",
    "                cur_l = train_l_sum / eval_period\n",
    "                print('epoch %d, batch %d, time %.2fs, train loss %.2f'\n",
    "                      % (epoch, batch_i, time.time() - start_time, cur_l.asscalar()))\n",
    "                train_l_sum = 0\n",
    "                get_knn(token_to_idx, idx_to_token, embedding, 5, example_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, batch 100, time 1.54s, train loss 0.40\n",
      "Closest tokens to \"president\": chief, tends, quebec, spy, c.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, batch 200, time 3.10s, train loss 0.33\n",
      "Closest tokens to \"president\": chief, o., lauder, officer, c.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, batch 300, time 4.43s, train loss 0.32\n",
      "Closest tokens to \"president\": o., robert, chief, shapiro, lauder\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, batch 400, time 5.77s, train loss 0.31\n",
      "Closest tokens to \"president\": succeeding, s., chief, o., michael\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, batch 500, time 7.09s, train loss 0.31\n",
      "Closest tokens to \"president\": succeeding, chief, joseph, walter, s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, batch 600, time 8.41s, train loss 0.31\n",
      "Closest tokens to \"president\": chief, succeeding, officer, named, bank-holding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, batch 700, time 9.76s, train loss 0.31\n",
      "Closest tokens to \"president\": chief, succeeding, s., officer, tucker\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, batch 100, time 1.65s, train loss 0.28\n",
      "Closest tokens to \"president\": conway, succeeding, chief, tucker, s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, batch 200, time 2.95s, train loss 0.27\n",
      "Closest tokens to \"president\": chief, tucker, s., conway, succeeding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, batch 300, time 4.27s, train loss nan\n",
      "Closest tokens to \"president\": <unk>, N, years, old, will\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, batch 400, time 5.59s, train loss nan\n",
      "Closest tokens to \"president\": <unk>, N, years, old, will\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, batch 500, time 6.92s, train loss nan\n",
      "Closest tokens to \"president\": <unk>, N, years, old, will\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, batch 600, time 8.24s, train loss nan\n",
      "Closest tokens to \"president\": <unk>, N, years, old, will\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, batch 700, time 9.58s, train loss nan\n",
      "Closest tokens to \"president\": <unk>, N, years, old, will\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, batch 100, time 1.65s, train loss nan\n",
      "Closest tokens to \"president\": <unk>, N, years, old, will\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, batch 200, time 2.94s, train loss nan\n",
      "Closest tokens to \"president\": <unk>, N, years, old, will\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, batch 300, time 4.28s, train loss nan\n",
      "Closest tokens to \"president\": <unk>, N, years, old, will\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, batch 400, time 5.60s, train loss nan\n",
      "Closest tokens to \"president\": <unk>, N, years, old, will\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, batch 500, time 6.94s, train loss nan\n",
      "Closest tokens to \"president\": <unk>, N, years, old, will\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, batch 600, time 8.26s, train loss nan\n",
      "Closest tokens to \"president\": <unk>, N, years, old, will\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, batch 700, time 9.60s, train loss nan\n",
      "Closest tokens to \"president\": <unk>, N, years, old, will\n"
     ]
    }
   ],
   "source": [
    "train_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] word2vec 工具 . https://code.google.com/archive/p/word2vec/\n",
    "\n",
    "[2] Mikolov, Tomas, et al. “Distributed representations of words and phrases and their compositionality.” Advances in neural information processing systems. 2013.[2] Mikolov, Tomas, et al. “Distributed representations of words and phrases and their compositionality.” Advances in neural information processing systems. 2013."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}