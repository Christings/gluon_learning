{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 门控循环单元（GRU）\n",
    "\n",
    "上一节介绍了循环神经网络中的梯度计算方法。我们发现，循环神经网络的梯度可能会衰减或爆炸。虽然裁剪梯度可以应对梯度爆炸，但无法解决梯度衰减的问题。给定一个时间序列，例如文本序列，循环神经网络在实际中较难捕捉时间步距离较大的词之间的依赖关系。\n",
    "\n",
    "门控循环神经网络（gated recurrent neural network）的提出，是为了更好地捕捉时间序列中时间步距离较大的依赖关系。其中，门控循环单元（gated recurrent unit，简称 GRU）是一种常用的门控循环神经网络 [1, 2]。我们将在下一节介绍另一种门控循环神经网络：长短期记忆。\n",
    "\n",
    "\n",
    "## 门控循环单元\n",
    "\n",
    "下面将介绍门控循环单元的设计。它引入了门的概念，从而修改了循环神经网络中隐藏状态的计算方式。输出层的设计不变。\n",
    "\n",
    "\n",
    "### 重置门和更新门\n",
    "\n",
    "假设隐藏单元个数为 $h$，给定时间步 $t$ 的小批量输入 $\\boldsymbol{X}_t \\in \\mathbb{R}^{n \\times d}$（样本数为 $n$，输入个数为 $d$）和上一时间步隐藏状态 $\\boldsymbol{H}_{t-1} \\in \\mathbb{R}^{n \\times h}$。重置门（reset gate）$\\boldsymbol{R}_t \\in \\mathbb{R}^{n \\times h}$ 和更新门（update gate）$\\boldsymbol{Z}_t \\in \\mathbb{R}^{n \\times h}$ 的计算如下：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{R}_t = \\sigma(\\boldsymbol{X}_t \\boldsymbol{W}_{xr} + \\boldsymbol{H}_{t-1} \\boldsymbol{W}_{hr} + \\boldsymbol{b}_r),\\\\\n",
    "\\boldsymbol{Z}_t = \\sigma(\\boldsymbol{X}_t \\boldsymbol{W}_{xz} + \\boldsymbol{H}_{t-1} \\boldsymbol{W}_{hz} + \\boldsymbol{b}_z).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中 $\\boldsymbol{W}_{xr}, \\boldsymbol{W}_{xz} \\in \\mathbb{R}^{d \\times h}$ 和 $\\boldsymbol{W}_{hr}, \\boldsymbol{W}_{hz} \\in \\mathbb{R}^{h \\times h}$ 是权重参数，$\\boldsymbol{b}_r, \\boldsymbol{b}_z \\in \\mathbb{R}^{1 \\times h}$ 是偏移参数。激活函数 $\\sigma$ 是 sigmoid 函数。[“多层感知机”](../chapter_deep-learning-basics/mlp.md) 一节中介绍过，sigmoid 函数可以将元素的值变换到 0 和 1 之间。因此，重置门 $\\boldsymbol{R}_t$ 和更新门 $\\boldsymbol{Z}_t$ 中每个元素的值域都是 $[0, 1]$。\n",
    "\n",
    "我们可以通过元素值域在 $[0, 1]$ 的更新门和重置门来控制隐藏状态中信息的流动：这通常可以应用按元素乘法符 $\\odot$。\n",
    "\n",
    "\n",
    "### 候选隐藏状态\n",
    "\n",
    "接下来，时间步 $t$ 的候选隐藏状态 $\\tilde{\\boldsymbol{H}}_t \\in \\mathbb{R}^{n \\times h}$ 的计算使用了值域在 $[-1, 1]$ 的 tanh 函数做激活函数。它在之前描述的循环神经网络隐藏状态表达式的基础上，引入了重置门和按元素乘法：\n",
    "\n",
    "$$\\tilde{\\boldsymbol{H}}_t = \\text{tanh}(\\boldsymbol{X}_t \\boldsymbol{W}_{xh} + \\boldsymbol{R}_t \\odot \\boldsymbol{H}_{t-1} \\boldsymbol{W}_{hh} + \\boldsymbol{b}_h),$$\n",
    "\n",
    "其中 $\\boldsymbol{W}_{xh} \\in \\mathbb{R}^{d \\times h}$ 和 $\\boldsymbol{W}_{hh} \\in \\mathbb{R}^{h \\times h}$ 是权重参数，$\\boldsymbol{b}_h \\in \\mathbb{R}^{1 \\times h}$ 是偏移参数。需要注意的是，候选隐藏状态使用了重置门，从而控制包含时间序列历史信息的上一个时间步的隐藏状态如何流入当前时间步的候选隐藏状态。如果重置门近似 0，上一个隐藏状态将被丢弃。因此，重置门可以丢弃与预测未来无关的历史信息。\n",
    "\n",
    "\n",
    "### 隐藏状态\n",
    "\n",
    "最后，隐藏状态 $\\boldsymbol{H}_t \\in \\mathbb{R}^{n \\times h}$ 的计算使用更新门 $\\boldsymbol{Z}_t$ 来对上一时间步的隐藏状态 $\\boldsymbol{H}_{t-1}$ 和当前时间步的候选隐藏状态 $\\tilde{\\boldsymbol{H}}_t$ 做组合：\n",
    "\n",
    "$$\\boldsymbol{H}_t = \\boldsymbol{Z}_t \\odot \\boldsymbol{H}_{t-1}  + (1 - \\boldsymbol{Z}_t) \\odot \\tilde{\\boldsymbol{H}}_t.$$\n",
    "\n",
    "值得注意的是，更新门可以控制隐藏状态应该如何被包含当前时间步信息的候选隐藏状态所更新。假设更新门在时间步 $t'$ 到 $t$（$t' < t$）之间一直近似 1。那么，在时间步 $t'$ 到 $t$ 之间的输入信息几乎没有流入时间步 $t$ 的隐藏状态 $\\boldsymbol{H}_t$。\n",
    "实际上，这可以看作是较早时刻的隐藏状态 $\\boldsymbol{H}_{t'-1}$ 一直通过时间保存并传递至当前时间步 $t$。\n",
    "这个设计可以应对循环神经网络中的梯度衰减问题，并更好地捕捉时间序列中时间步距离较大的依赖关系。\n",
    "\n",
    "我们对门控循环单元的设计稍作总结：\n",
    "\n",
    "* 重置门有助于捕捉时间序列里短期的依赖关系。\n",
    "* 更新门有助于捕捉时间序列里长期的依赖关系。\n",
    "\n",
    "\n",
    "\n",
    "## 实验\n",
    "\n",
    "\n",
    "为了实现并展示门控循环单元，我们依然使用周杰伦歌词数据集来训练模型作词。这里除门控循环单元以外的实现已在 [“循环神经网络”](rnn.md) 一节中介绍。\n",
    "\n",
    "\n",
    "### 处理数据\n",
    "\n",
    "我们先读取并简单处理数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import gluonbook as gb\n",
    "from mxnet import nd\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('../data/jaychou_lyrics.txt.zip', 'r') as zin:\n",
    "    zin.extractall('../data/')\n",
    "with open('../data/jaychou_lyrics.txt', encoding='utf-8') as f:\n",
    "    corpus_chars = f.read()\n",
    "\n",
    "corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\n",
    "corpus_chars = corpus_chars[0:20000]\n",
    "idx_to_char = list(set(corpus_chars))\n",
    "char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
    "corpus_indices = [char_to_idx[char] for char in corpus_chars]\n",
    "vocab_size = len(char_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化模型参数\n",
    "\n",
    "以下部分对模型参数进行初始化。超参数 `num_hiddens` 定义了隐藏单元的个数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "ctx = gb.try_gpu()\n",
    "num_inputs = vocab_size\n",
    "num_hiddens = 256\n",
    "num_outputs = vocab_size\n",
    "\n",
    "def get_params():\n",
    "    # 更新门参数。\n",
    "    W_xz = nd.random_normal(scale=0.01, shape=(num_inputs, num_hiddens),\n",
    "                            ctx=ctx)\n",
    "    W_hz = nd.random_normal(scale=0.01, shape=(num_hiddens, num_hiddens),\n",
    "                            ctx=ctx)\n",
    "    b_z = nd.zeros(num_hiddens, ctx=ctx)\n",
    "    # 重置门参数。\n",
    "    W_xr = nd.random_normal(scale=0.01, shape=(num_inputs, num_hiddens),\n",
    "                            ctx=ctx)\n",
    "    W_hr = nd.random_normal(scale=0.01, shape=(num_hiddens, num_hiddens),\n",
    "                            ctx=ctx)\n",
    "    b_r = nd.zeros(num_hiddens, ctx=ctx)\n",
    "    # 候选隐藏状态参数。\n",
    "    W_xh = nd.random_normal(scale=0.01, shape=(num_inputs, num_hiddens),\n",
    "                            ctx=ctx)\n",
    "    W_hh = nd.random_normal(scale=0.01, shape=(num_hiddens, num_hiddens),\n",
    "                            ctx=ctx)\n",
    "    b_h = nd.zeros(num_hiddens, ctx=ctx)\n",
    "    # 输出层参数。\n",
    "    W_hy = nd.random_normal(scale=0.01, shape=(num_hiddens, num_outputs),\n",
    "                            ctx=ctx)\n",
    "    b_y = nd.zeros(num_outputs, ctx=ctx)\n",
    "\n",
    "    params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hy, b_y]\n",
    "    for param in params:\n",
    "        param.attach_grad()\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型\n",
    "\n",
    "下面根据门控循环单元的计算表达式定义模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [],
   "source": [
    "def gru_rnn(inputs, H, *params):\n",
    "    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hy, b_y = params\n",
    "    outputs = []\n",
    "    for X in inputs:        \n",
    "        Z = nd.sigmoid(nd.dot(X, W_xz) + nd.dot(H, W_hz) + b_z)\n",
    "        R = nd.sigmoid(nd.dot(X, W_xr) + nd.dot(H, W_hr) + b_r)\n",
    "        H_tilda = nd.tanh(nd.dot(X, W_xh) + R * nd.dot(H, W_hh) + b_h)\n",
    "        H = Z * H + (1 - Z) * H_tilda\n",
    "        Y = nd.dot(H, W_hy) + b_y\n",
    "        outputs.append(Y)\n",
    "    return (outputs, H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型并创作歌词\n",
    "\n",
    "设置好超参数后，我们将训练模型并跟据前缀“分开”和“不分开”分别创作长度为 100 个字符的一段歌词。我们每过 30 个迭代周期便根据当前训练的模型创作一段歌词。训练模型时采用了相邻采样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 30, perplexity 116.174408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -   分开 我不要我的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -   不分开 我不要我的可爱女人 我想能这不要 我不要我不要 我不要我不要 我不要我不要 我不要我不要 我不要我不要 我不要我不要 我不要我不要 我不要我不要 我不要我不要 我不要我不要 我不要我不要 我不要我不要 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 60, perplexity 10.390560\n",
      " -   分开 是想开了了 有一直杨气 你在那里 是谁在练落 不爽 苦不去 再想就停不活 你说  我连著有了天 我该好好生活 我该好好生活 我该好好生活 我该好好生活 我该好好生活 我该好好生活 我该好好生活 我该好好\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -   不分开 我想要这样样 我右拳打开了天 化身为龙 那大地心脏汹涌 不安跳动 全世界 的表情只剩下一种 等待英雄 我就是那条龙 我右拳打开了天 化身为龙 那大地心脏汹涌 不安跳动 全世界 的表情只剩下一种 等待英雄\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 90, perplexity 1.971738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -   分开 一直好 我有一直是我听错错错错                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -   不分开 睡不开 别让我 印地安 的灵魂 翻滚 停止忿恨 永无止尽的过程 让我们 半兽人 的灵魂 单纯 对远古存在的神 用谦卑的身份 再也没有纯白的灵魂 自人类堕落为半兽人 我开始使用第一人称 记录眼前所有的发生\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 120, perplexity 1.140207\n",
      " -   分开 干什么跟怎么举手飞到 难道大大风 我们要向他看齐 我不想就这样打三事 趁录为人们里自自般 为什么这种速度你追不到 不好笑不好笑不好笑…… 这第一名到底要多强 不用问 一定有人向你挑战 到底还要过多少关 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -   不分开 我想要 你怎么抄我球 你说啊 是不是你不想活 说你怎么面对我 甩开球我满腔的怒火 我想揍你已经很久 别想躲 说你眼睛看着我 别发抖 快给我抬起头 有话去对医药箱说 别怪我 别怪我 说你怎么面对我 甩开球\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 150, perplexity 1.063079\n",
      " -   分开 干什么调武过 一直在我留里 小小壁的梦 像幅泼墨的山水画 唐朝千年的风沙 现在还在刮 那天 我翻阅字典 查什么字眼 形容一件事 很遥远 天边 是否在海角对面 直到九岁才知道浪费时间 这茶桌樟木的横切面 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -   不分开 我想要 你怎么我手你 你爱我 你爱我 你不想再想要我 爱爱你的太傲 这样不要得紧  我的世界被你看要的想多 我的古你防狂牵了暴了W存出了 活著我这种来这样 杂讯太多讯号  就连风吹都要干扰  可是你不想\n"
     ]
    }
   ],
   "source": [
    "get_inputs = gb.to_onehot\n",
    "num_epochs = 150\n",
    "num_steps = 35\n",
    "batch_size = 32\n",
    "lr = 0.25\n",
    "clipping_theta = 5\n",
    "prefixes = [' 分开 ', ' 不分开 ']\n",
    "pred_period = 30\n",
    "pred_len = 100\n",
    "\n",
    "gb.train_and_predict_rnn(gru_rnn, False, num_epochs, num_steps, num_hiddens,\n",
    "                         lr, clipping_theta, batch_size, vocab_size,\n",
    "                         pred_period, pred_len, prefixes, get_params,\n",
    "                         get_inputs, ctx, corpus_indices, idx_to_char,\n",
    "                         char_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 门控循环神经网络的可以更好地捕捉时间序列中时间步距离较大的依赖关系，它包括门控循环单元和长短期记忆。\n",
    "* 门控循环单元引入了门的概念，从而修改了循环神经网络中隐藏状态的计算方式。它包括重置门、更新门、候选隐藏状态和隐藏状态。\n",
    "* 重置门有助于捕捉时间序列里短期的依赖关系。\n",
    "* 更新门有助于捕捉时间序列里长期的依赖关系。\n",
    "\n",
    "\n",
    "## 练习\n",
    "\n",
    "* 假设时间步 $t' < t$。如果我们只希望用时间步 $t'$ 的输入来预测时间步 $t$ 的输出，每个时间步的重置门和更新门的值最好是多少？\n",
    "* 调调超参数，观察并分析对运行时间、困惑度以及创作歌词的结果造成的影响。\n",
    "* 在相同条件下，比较门控循环单元和循环神经网络的运行时间。\n",
    "\n",
    "\n",
    "## 扫码直达 [ 讨论区 ](https://discuss.gluon.ai/t/topic/4042)\n",
    "\n",
    "![](../img/qr_gru.svg)\n",
    "\n",
    "## 参考文献\n",
    "\n",
    "[1] Cho, K., Van Merri ë nboer, B., Bahdanau, D., & Bengio, Y. (2014). On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259.\n",
    "\n",
    "[2] Chung, J., Gulcehre, C., Cho, K., & Bengio, Y. (2014). Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}