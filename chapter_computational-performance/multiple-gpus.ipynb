{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多 GPU 计算\n",
    "\n",
    "本教程我们将展示如何使用多个 GPU 计算，例如使用多个 GPU 训练模型。正如你期望的那样，运行本节中的程序需要至少两块 GPU。事实上，一台机器上安装多块 GPU 非常常见。这是因为主板上通常会有多个 PCIe 插槽。如果正确安装了 NVIDIA 驱动，我们可以通过 `nvidia-smi` 命令来查看当前机器上的全部 GPU。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jul 19 18:55:47 2018       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 375.26                 Driver Version: 375.26                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla M60           On   | 0000:00:1D.0     Off |                    0 |\r\n",
      "| N/A   34C    P0    37W / 150W |    298MiB /  7612MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla M60           On   | 0000:00:1E.0     Off |                    0 |\r\n",
      "| N/A   42C    P0    37W / 150W |    289MiB /  7612MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID  Type  Process name                               Usage      |\r\n",
      "|=============================================================================|\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 [“自动并行计算”](auto-parallelism.md) 一节里，我们介绍过，大部分的运算可以使用所有的 CPU 的全部计算资源，或者单个 GPU 的全部计算资源。但如果使用多个 GPU 训练模型，我们仍然需要实现相应的算法。这些算法中最常用的叫做数据并行。\n",
    "\n",
    "\n",
    "## 数据并行\n",
    "\n",
    "数据并行目前是深度学习里使用最广泛的将模型训练任务划分到多个 GPU 的办法。回忆一下我们在 [“梯度下降和随机梯度下降”](../chapter_optimization/gd-sgd.md) 一节中介绍的使用优化算法训练模型的过程。下面我们就以小批量随机梯度下降为例来介绍数据并行是如何工作的。\n",
    "\n",
    "假设一台机器上有 $k$ 个 GPU。给定需要训练的模型，每个 GPU 将分别独立维护一份完整的模型参数。在模型训练的任意一次迭代中，给定一个小批量，我们将该批量中的样本划分成 $k$ 份并分给每个 GPU 一份。然后，每个 GPU 将分别根据自己分到的训练数据样本和自己维护的模型参数计算模型参数的梯度。\n",
    "接下来，我们把 $k$ 个 GPU 上分别计算得到的梯度相加，从而得到当前的小批量梯度。\n",
    "之后，每个 GPU 都使用这个小批量梯度分别更新自己维护的那一份完整的模型参数。\n",
    "\n",
    "为了从零开始实现多 GPU 训练中的数据并行，让我们先导入需要的包或模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import gluonbook as gb\n",
    "import mxnet as mx\n",
    "from mxnet import autograd, nd\n",
    "from mxnet.gluon import loss as gloss\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型\n",
    "\n",
    "我们使用 [“卷积神经网络：LeNet”](../chapter_convolutional-neural-networks/lenet.md) 一节里介绍的 LeNet 来作为本节的样例模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "# 初始化模型参数。\n",
    "scale = 0.01\n",
    "W1 = nd.random.normal(scale=scale, shape=(20, 1, 3, 3))\n",
    "b1 = nd.zeros(shape=20)\n",
    "W2 = nd.random.normal(scale=scale, shape=(50, 20, 5, 5))\n",
    "b2 = nd.zeros(shape=50)\n",
    "W3 = nd.random.normal(scale=scale, shape=(800, 128))\n",
    "b3 = nd.zeros(shape=128)\n",
    "W4 = nd.random.normal(scale=scale, shape=(128, 10))\n",
    "b4 = nd.zeros(shape=10)\n",
    "params = [W1, b1, W2, b2, W3, b3, W4, b4]\n",
    "\n",
    "# 定义模型。\n",
    "def lenet(X, params):\n",
    "    h1_conv = nd.Convolution(data=X, weight=params[0], bias=params[1],\n",
    "                             kernel=(3, 3), num_filter=20)\n",
    "    h1_activation = nd.relu(h1_conv)\n",
    "    h1 = nd.Pooling(data=h1_activation, pool_type='avg', kernel=(2, 2),\n",
    "                    stride=(2, 2))\n",
    "    h2_conv = nd.Convolution(data=h1, weight=params[2], bias=params[3],\n",
    "                             kernel=(5, 5), num_filter=50)\n",
    "    h2_activation = nd.relu(h2_conv)\n",
    "    h2 = nd.Pooling(data=h2_activation, pool_type='avg', kernel=(2, 2),\n",
    "                    stride=(2, 2))\n",
    "    h2 = nd.flatten(h2)\n",
    "    h3_linear = nd.dot(h2, params[4]) + params[5]\n",
    "    h3 = nd.relu(h3_linear)\n",
    "    y_hat = nd.dot(h3, params[6]) + params[7]\n",
    "    return y_hat\n",
    "\n",
    "# 交叉熵损失函数。\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多 GPU 之间同步数据\n",
    "\n",
    "我们需要实现一些多 GPU 之间同步数据的辅助函数。下面函数将模型参数复制到某个特定 GPU 并初始化梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "def get_params(params, ctx):\n",
    "    new_params = [p.copyto(ctx) for p in params]\n",
    "    for p in new_params:\n",
    "        p.attach_grad()\n",
    "    return new_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "试一试把 `params` 复制到 `mx.gpu(0)` 上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b1 weight: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "<NDArray 20 @gpu(0)>\n",
      "b1 grad: \n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "<NDArray 20 @gpu(0)>\n"
     ]
    }
   ],
   "source": [
    "new_params = get_params(params, mx.gpu(0))\n",
    "print('b1 weight:', new_params[1])\n",
    "print('b1 grad:', new_params[1].grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定分布在多个 GPU 之间的数据。以下函数可以把各个 GPU 上的数据加起来，然后再广播到所有 GPU 上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [],
   "source": [
    "def allreduce(data):\n",
    "    for i in range(1, len(data)):\n",
    "        data[0][:] += data[i].copyto(data[0].context)\n",
    "    for i in range(1, len(data)):\n",
    "        data[0].copyto(data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单测试一下 `allreduce` 函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before allreduce: [\n",
      "[[ 1.  1.]]\n",
      "<NDArray 1x2 @gpu(0)>, \n",
      "[[ 2.  2.]]\n",
      "<NDArray 1x2 @gpu(1)>]\n",
      "after allreduce: [\n",
      "[[ 3.  3.]]\n",
      "<NDArray 1x2 @gpu(0)>, \n",
      "[[ 3.  3.]]\n",
      "<NDArray 1x2 @gpu(1)>]\n"
     ]
    }
   ],
   "source": [
    "data = [nd.ones((1,2), ctx=mx.gpu(i)) * (i + 1) for i in range(2)]\n",
    "print('before allreduce:', data)\n",
    "allreduce(data)\n",
    "print('after allreduce:', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定一个批量的数据样本，以下函数可以划分它们并复制到各个 GPU 上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "def split_and_load(data, ctx):\n",
    "    n, k = data.shape[0], len(ctx)\n",
    "    m = n // k\n",
    "    assert m * k == n, '# examples is not divided by # devices.'\n",
    "    return [data[i * m: (i + 1) * m].as_in_context(ctx[i]) for i in range(k)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们试着用 `split_and_load` 函数将 6 个数据样本平均分给 2 个 GPU。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  \n",
      "[[  0.   1.   2.   3.]\n",
      " [  4.   5.   6.   7.]\n",
      " [  8.   9.  10.  11.]\n",
      " [ 12.  13.  14.  15.]\n",
      " [ 16.  17.  18.  19.]\n",
      " [ 20.  21.  22.  23.]]\n",
      "<NDArray 6x4 @cpu(0)>\n",
      "load into [gpu(0), gpu(1)]\n",
      "output: [\n",
      "[[  0.   1.   2.   3.]\n",
      " [  4.   5.   6.   7.]\n",
      " [  8.   9.  10.  11.]]\n",
      "<NDArray 3x4 @gpu(0)>, \n",
      "[[ 12.  13.  14.  15.]\n",
      " [ 16.  17.  18.  19.]\n",
      " [ 20.  21.  22.  23.]]\n",
      "<NDArray 3x4 @gpu(1)>]\n"
     ]
    }
   ],
   "source": [
    "batch = nd.arange(24).reshape((6, 4))\n",
    "ctx = [mx.gpu(0), mx.gpu(1)]\n",
    "splitted = split_and_load(batch, ctx)\n",
    "print('input: ', batch)\n",
    "print('load into', ctx)\n",
    "print('output:', splitted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单个小批量上的多 GPU 训练\n",
    "\n",
    "现在我们可以实现单个小批量上的多 GPU 训练了。它的实现主要依据本节介绍的数据并行方法。我们将使用刚刚定义的多 GPU 之间同步数据的辅助函数，例如 `split_and_load` 和 `allreduce`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [],
   "source": [
    "def train_batch(X, y, gpu_params, ctx, lr):\n",
    "    # 当 ctx 包含多个 GPU 时，划分小批量数据样本并复制到各个 GPU 上。\n",
    "    gpu_Xs = split_and_load(X, ctx)\n",
    "    gpu_ys = split_and_load(y, ctx)\n",
    "    # 在各个 GPU 上计算损失。\n",
    "    with autograd.record():\n",
    "        ls = [loss(lenet(gpu_X, gpu_W), gpu_y) \n",
    "              for gpu_X, gpu_y, gpu_W in zip(gpu_Xs, gpu_ys, gpu_params)]\n",
    "    # 在各个 GPU 上反向传播。\n",
    "    for l in ls:\n",
    "        l.backward()\n",
    "    # 把各个 GPU 上的梯度加起来，然后再广播到所有 GPU 上。\n",
    "    for i in range(len(gpu_params[0])):\n",
    "        allreduce([gpu_params[c][i].grad for c in range(len(ctx))])\n",
    "    # 在各个 GPU 上更新自己维护的那一份完整的模型参数。\n",
    "    for param in gpu_params:\n",
    "        gb.sgd(param, lr, X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练函数\n",
    "\n",
    "现在我们可以定义训练函数。这里的训练函数和之前章节里的训练函数稍有不同。例如，在这里我们需要依据本节介绍的数据并行，将完整的模型参数复制到多个 GPU 上，并在每次迭代时对单个小批量上进行多 GPU 训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [],
   "source": [
    "def train(num_gpus, batch_size, lr):\n",
    "    train_iter, test_iter = gb.load_data_fashion_mnist(batch_size)\n",
    "    ctx = [mx.gpu(i) for i in range(num_gpus)]\n",
    "    print('running on:', ctx)\n",
    "    # 将模型参数复制到 num_gpus 个 GPU 上。\n",
    "    gpu_params = [get_params(params, c) for c in ctx]\n",
    "    for epoch in range(1, 6):\n",
    "        start = time()\n",
    "        for X, y in train_iter:\n",
    "            # 对单个小批量上进行多 GPU 训练。\n",
    "            train_batch(X, y, gpu_params, ctx, lr)\n",
    "        nd.waitall()\n",
    "        print('epoch %d, time: %.1f sec' % (epoch, time() - start))\n",
    "        # 在 GPU0 上验证模型。\n",
    "        net = lambda x: lenet(x, gpu_params[0])\n",
    "        test_acc = gb.evaluate_accuracy(test_iter, net, ctx[0])\n",
    "        print('validation accuracy: %.4f' % test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们先使用一个 GPU 来训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on: [gpu(0)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, time: 2.3 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy: 0.1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, time: 1.9 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy: 0.7259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, time: 1.8 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy: 0.7708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, time: 1.8 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy: 0.7681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, time: 1.8 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy: 0.8049\n"
     ]
    }
   ],
   "source": [
    "train(num_gpus=1, batch_size=256, lr=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们先使用 2 个 GPU 来训练。我们将批量大小也增加一倍，以使得 GPU 的计算资源能够得到较充分利用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on: [gpu(0), gpu(1)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, time: 1.5 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy: 0.1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, time: 1.3 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy: 0.1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, time: 1.3 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy: 0.1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, time: 1.3 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy: 0.2607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, time: 1.5 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy: 0.6842\n"
     ]
    }
   ],
   "source": [
    "train(num_gpus=2, batch_size=512, lr=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于批量大小增加了一倍，每个迭代周期的迭代次数减小了一半。因此，我们观察到每个迭代周期的耗时比单 GPU 训练时少了近一半。但由于总体迭代次数的减少，模型在验证数据集上的精度略有下降。这很可能是由于训练不够充分造成的。因此，多 GPU 训练时，我们可以适当增加迭代周期使训练较充分。\n",
    "\n",
    "\n",
    "\n",
    "## 小结\n",
    "\n",
    "* 我们可以使用数据并行更充分地利用多个 GPU 的计算资源，实现多 GPU 训练模型。\n",
    "\n",
    "## 练习\n",
    "\n",
    "* 在本节实验中，试一试不同的迭代周期、批量大小和学习率。\n",
    "* 将本节实验的模型预测部分改为用多 GPU 预测。\n",
    "\n",
    "\n",
    "## 扫码直达 [ 讨论区 ](https://discuss.gluon.ai/t/topic/1884)\n",
    "\n",
    "![](../img/qr_multiple-gpus.svg)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}