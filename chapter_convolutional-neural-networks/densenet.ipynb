{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 稠密连接网络（DenseNet）\n",
    "\n",
    "ResNet 中的跨层连接设计引申出了数个后续工作。这一节我们介绍其中的一个：稠密连接网络（DenseNet） [1]。 它与 ResNet 的主要区别如下图演示。\n",
    "\n",
    "![ResNet（左）对比 DenseNet（右）。](../img/densenet.svg)\n",
    "\n",
    "主要区别在于，DenseNet 里层 B 的输出不是像 ResNet 那样和层 A 的输出相加，而是在通道维上合并，这样层 A 的输出可以不受影响的进入上面的神经层。这个设计里，层 A 直接跟上面的所有层连接在了一起，这也是它被称为“稠密连接“的原因。\n",
    "\n",
    "DenseNet 的主要构建模块是稠密块和过渡块，前者定义了输入和输出是如何合并的，后者则用来控制通道数不要过大。\n",
    "\n",
    "## 稠密块\n",
    "\n",
    "DeseNet 使用了 ResNet 改良版的“批量归一化、激活和卷积”结构（参见上一节习题），我们首先在 `conv_block` 函数里实现这个结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import gluonbook as gb\n",
    "from mxnet import nd, gluon, init\n",
    "from mxnet.gluon import loss as gloss, nn\n",
    "\n",
    "def conv_block(num_channels):\n",
    "    blk = nn.Sequential()\n",
    "    blk.add(nn.BatchNorm(), nn.Activation('relu'),\n",
    "            nn.Conv2D(num_channels, kernel_size=3, padding=1))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "稠密块由多个 `conv_block` 组成，每块使用相同的输出通道数。但在正向传播时，我们将每块的输出在通道维上同其输出合并进入下一个块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "class DenseBlock(nn.Block):\n",
    "    def __init__(self, num_convs, num_channels, **kwargs):\n",
    "        super(DenseBlock, self).__init__(**kwargs)\n",
    "        self.net = nn.Sequential()\n",
    "        for _ in range(num_convs):\n",
    "            self.net.add(conv_block(num_channels))\n",
    "\n",
    "    def forward(self, X):\n",
    "        for blk in self.net:\n",
    "            Y = blk(X)\n",
    "            # 在通道维上将输入和输出合并。\n",
    "            X = nd.concat(X, Y, dim=1)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面例子中我们定义一个有两个输出通道数为 10 的卷积块，使用通道数为 3 的输入时，我们会得到通道数为 $3+2\\times 10=23$ 的输出。卷积块的通道数控制了输出通道数相对于输入通道数的增长，因此也被称为增长率（growth rate）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 10, 8, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = DenseBlock(2, 10)\n",
    "blk.initialize()\n",
    "X = nd.random.uniform(shape=(4,3,8,8))\n",
    "Y = blk(X)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 过渡块\n",
    "\n",
    "由于每个稠密块都会带来通道数的增加。使用过多则会导致过于复杂的模型。过渡块（transition block）则用来控制模型复杂度。它通过 $1\\times1$ 卷积层来减小通道数，同时使用步幅为 2 的平均池化层来将高宽减半来进一步降低复杂度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "def transition_block(num_channels):\n",
    "    blk = nn.Sequential()\n",
    "    blk.add(nn.BatchNorm(), nn.Activation('relu'),\n",
    "            nn.Conv2D(num_channels, kernel_size=1),\n",
    "            nn.AvgPool2D(pool_size=2, strides=2))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们对前面的稠密块的输出使用通道数为 10 的过渡块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 10, 4, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = transition_block(10)\n",
    "blk.initialize()\n",
    "blk(Y).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DenseNet 模型\n",
    "\n",
    "DenseNet 首先使用跟 ResNet 一样的单卷积层和最大池化层："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Conv2D(64, kernel_size=7, strides=2, padding=3),\n",
    "        nn.BatchNorm(), nn.Activation('relu'),\n",
    "        nn.MaxPool2D(pool_size=3, strides=2, padding=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似于 ResNet 接下来使用的四个基于残差块，DenseNet 使用的是四个稠密块。同 ResNet 一样我们可以设置每个稠密块使用多少个卷积层，这里我们设成 4，跟上一节的 ResNet 18 保持一致。稠密块里的卷积层通道数（既增长率）设成 32，所以每个稠密块将增加 128 通道。\n",
    "\n",
    "ResNet 里通过步幅为 2 的残差块来在每个模块之间减小高宽，这里我们则是使用过渡块来减半高宽，并且减半输入通道数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "# 当前的数据通道数。\n",
    "num_channels = 64\n",
    "growth_rate = 32\n",
    "num_convs_in_dense_blocks = [4, 4, 4, 4]\n",
    "\n",
    "for i, num_convs in enumerate(num_convs_in_dense_blocks):\n",
    "    net.add(DenseBlock(num_convs, growth_rate))\n",
    "    # 上一个稠密的输出通道数。\n",
    "    num_channels += num_convs * growth_rate\n",
    "    # 在稠密块之间加入通道数减半的过渡块。\n",
    "    if i != len(num_convs_in_dense_blocks) - 1:\n",
    "        net.add(transition_block(num_channels // 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后同 ResNet 一样我们接上全局池化层和全连接层来输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.add(nn.BatchNorm(), nn.Activation('relu'), nn.GlobalAvgPool2D(),\n",
    "        nn.Dense(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取数据并训练\n",
    "\n",
    "因为这里我们使用了比较深的网络，所以我们进一步把输入减少到 $32\\times 32$ 来训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on gpu(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.5511, train acc 0.819, test acc 0.857, time 47.7 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss 0.2916, train acc 0.897, test acc 0.875, time 50.1 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, loss 0.2406, train acc 0.912, test acc 0.899, time 48.0 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, loss 0.2124, train acc 0.923, test acc 0.906, time 44.9 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, loss 0.1907, train acc 0.931, test acc 0.923, time 45.1 sec\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "num_epochs = 5\n",
    "batch_size = 256\n",
    "ctx = gb.try_gpu()\n",
    "net.initialize(force_reinit=True, ctx=ctx, init=init.Xavier())\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "train_iter, test_iter = gb.load_data_fashion_mnist(batch_size=batch_size,\n",
    "                                                   resize=96)\n",
    "gb.train_ch5(net, train_iter, test_iter, loss, batch_size, trainer, ctx,\n",
    "             num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 不同于 ResNet 中将输入加在输出上完成跨层连接，DenseNet 在通道维上合并输入和输出来使得底部神经层能跟其上面所有层连接起来。\n",
    "\n",
    "## 练习\n",
    "\n",
    "- DesNet 论文中提交的一个优点是其模型参数比 ResNet 更小，这是为什么？\n",
    "- DesNet 被人诟病的一个问题是内存消耗过多。真的会这样吗？可以把输入换成 $224\\times 224$，来看看实际（GPU）内存消耗。\n",
    "- 实现 [1] 中的表 1 提出的各个 DenseNet 版本。\n",
    "\n",
    "## 扫码直达 [ 讨论区 ](https://discuss.gluon.ai/t/topic/1664)\n",
    "\n",
    "![](../img/qr_densenet-gluon.svg)\n",
    "\n",
    "## 参考文献\n",
    "\n",
    "[1] Huang, G., Liu, Z., Weinberger, K. Q., & van der Maaten, L. (2017). Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (Vol. 1, No. 2)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}