{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网络中的网络（NiN）\n",
    "\n",
    "前面小节里我们看到 LeNet、AlexNet 和 VGG 均由两个部分组成：以卷积层构成的模块充分抽取空间特征，然后以全连接层构成的模块来输出最终分类结果。AlexNet 和 VGG 对 LeNet 的改进主要在于如何加深加宽这两个模块。这一节我们介绍网络中的网络（NiN）[1]。它提出了另外一个思路，即串联多个由卷积层和“全连接”层构成的小网络来构建一个深层网络。\n",
    "\n",
    "## NiN 块\n",
    "\n",
    "我们知道卷积层的输入和输出都是四维数组，而全连接层则是二维数组。如果想在全连接层后再接上卷积层，则需要将其输出转回到四维。回忆在 [“多输入和输出通道”](channels.md) 这一小节里介绍的 $1\\times 1$ 卷积，它可以看成将空间维（高和宽）上每个元素当做样本，并作用在通道维上的全连接层。NiN 使用 $1\\times 1$ 卷积层来替代全连接层使得空间信息能够自然传递到后面的层去。下图对比了 NiN 同 AlexNet 和 VGG 等网络的主要区别。\n",
    "\n",
    "![ 对比 NiN（右）和其他（左）。](../img/nin.svg)\n",
    "\n",
    "NiN 中的一个基础块由一个卷积层外加两个充当全连接层的 $1\\times 1$ 卷积层构成。第一个卷积层我们可以设置它的超参数，而第二和第三卷积层则使用固定超参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "import gluonbook as gb\n",
    "from mxnet import nd, gluon, init\n",
    "from mxnet.gluon import loss as gloss, nn\n",
    "\n",
    "def nin_block(num_channels, kernel_size, strides, padding):\n",
    "    blk = nn.Sequential()\n",
    "    blk.add(nn.Conv2D(num_channels, kernel_size,\n",
    "                      strides, padding, activation='relu'),\n",
    "            nn.Conv2D(num_channels, kernel_size=1, activation='relu'),\n",
    "            nn.Conv2D(num_channels, kernel_size=1, activation='relu'))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NiN 模型\n",
    "\n",
    "NiN 紧跟 AlexNet 后提出，所以它的卷积层设定跟 Alexnet 类似。它使用窗口分别为 $11\\times 11$、$5\\times 5$ 和 $3\\times 3$ 的卷积层，输出通道数也与之相同。卷积层后跟步幅为 2 的 $3\\times 3$ 最大池化层。\n",
    "\n",
    "除了使用 NiN 块外，NiN 还有一个重要的跟 AlexNet 不同的地方：NiN 去掉了最后的三个全连接层，取而代之的是使用输出通道数等于标签类数的卷积层，然后使用一个窗口为输入高宽的平均池化层来将每个通道里的数值平均成一个标量直接用于分类。这个设计好处是可以显著的减小模型参数大小，从而能很好的避免过拟合，但它也可能会造成训练时收敛变慢。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(\n",
    "    nin_block(96, kernel_size=11, strides=4, padding=0),\n",
    "    nn.MaxPool2D(pool_size=3, strides=2),\n",
    "    nin_block(256, kernel_size=5, strides=1, padding=2),\n",
    "    nn.MaxPool2D(pool_size=3, strides=2),\n",
    "    nin_block(384, kernel_size=3, strides=1, padding=1),\n",
    "    nn.MaxPool2D(pool_size=3, strides=2), nn.Dropout(0.5),\n",
    "    # 标签类数是 10。\n",
    "    nin_block(10, kernel_size=3, strides=1, padding=1),\n",
    "    # 全局平均池化层将窗口形状自动设置成输出的高和宽。\n",
    "    nn.GlobalAvgPool2D(),\n",
    "    # 将四维的输出转成二维的输出，其形状为（批量大小，10）。\n",
    "    nn.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们构建一个数据来查看每一层的输出大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential1 output shape:\t (1, 96, 54, 54)\n",
      "pool0 output shape:\t (1, 96, 26, 26)\n",
      "sequential2 output shape:\t (1, 256, 26, 26)\n",
      "pool1 output shape:\t (1, 256, 12, 12)\n",
      "sequential3 output shape:\t (1, 384, 12, 12)\n",
      "pool2 output shape:\t (1, 384, 5, 5)\n",
      "dropout0 output shape:\t (1, 384, 5, 5)\n",
      "sequential4 output shape:\t (1, 10, 5, 5)\n",
      "pool3 output shape:\t (1, 10, 1, 1)\n",
      "flatten0 output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X = nd.random.uniform(shape=(1, 1, 224, 224))\n",
    "net.initialize()\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.name, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取数据并训练\n",
    "\n",
    "NiN 的训练与 Alexnet 和 VGG 类似，但一般使用更大的学习率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on gpu(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 2.2250, train acc 0.186, test acc 0.221, time 98.6 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss 1.5570, train acc 0.441, test acc 0.615, time 96.0 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, loss 0.9230, train acc 0.662, test acc 0.744, time 96.0 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, loss 0.6401, train acc 0.766, test acc 0.771, time 96.0 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, loss 0.5273, train acc 0.811, test acc 0.841, time 96.0 sec\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "num_epochs = 5\n",
    "batch_size = 128\n",
    "ctx = gb.try_gpu()\n",
    "net.initialize(force_reinit=True, ctx=ctx, init=init.Xavier())\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "train_iter, test_iter = gb.load_data_fashion_mnist(batch_size=batch_size,\n",
    "                                                   resize=224)\n",
    "gb.train_ch5(net, train_iter, test_iter, loss, batch_size, trainer, ctx,\n",
    "             num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "NiN 提供了两个重要的设计思路：\n",
    "\n",
    "- 重复使用由卷积层和代替全连接层的 $1\\times 1$ 卷积层构成的基础块来构建深层网络；\n",
    "- 去除了容易造成过拟合的全连接输出层，而是替换成输出通道数等于标签类数的卷积层和全局平均池化层。\n",
    "\n",
    "虽然因为精度和收敛速度等问题 NiN 并没有像本章中介绍的其他网络那么被广泛使用，但 NiN 的设计思想影响了后面的一系列网络的设计。\n",
    "\n",
    "## 练习\n",
    "\n",
    "- 多用几个迭代周期来观察网络收敛速度。\n",
    "- 为什么 NiN 块里要有两个 $1\\times 1$ 卷积层，去除一个看看？\n",
    "\n",
    "## 扫码直达 [ 讨论区 ](https://discuss.gluon.ai/t/topic/1661)\n",
    "\n",
    "![](../img/qr_nin-gluon.svg)\n",
    "\n",
    "## 参考文献\n",
    "\n",
    "[1] Lin, M., Chen, Q., & Yan, S. (2013). Network in network. arXiv preprint arXiv:1312.4400."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}