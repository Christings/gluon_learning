{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 样式迁移\n",
    "\n",
    "喜欢拍照的同学可能都接触过滤镜，它们能改变照片的颜色风格，可以使得风景照更加锐利或者人像更加美白。但一个滤镜通常只能改变照片的某个方面，达到想要的风格经常需要大量组合尝试，其复杂程度不亚于模型调参。\n",
    "\n",
    "本小节我们将介绍如何使用神经网络来自动化这个过程 [1]。这里我们需要两张输入图片，一张是内容图片，另一张是样式图片，我们将使用神经网络修改内容图片使得其样式接近样式图片。下图中的内容图片为作者在西雅图郊区的雷尼尔山（mountain rainier）拍摄风景照，样式图片则为一副主题为秋天橡树的油画，其合成图片在保留了内容图片中物体主体形状的情况下加入了样式图片的油画笔触，同时也让整体颜色更加鲜艳。\n",
    "\n",
    "![ 样式迁移。](../img/style-transfer.svg)\n",
    "\n",
    "\n",
    "使用神经网络进行样式迁移的过程如下图所示。在图中我们选取一个有三个卷积层的神经网络为例，来提取特征。对于样式图片，我们选取第一和第三层输出作为样式特征。对于内容图片则选取第二层输出作为内容特征。给定一个合成图片的初始值，我们通过不断的迭代直到其与样式图片输入到同一神经网络时，第一和第三层输出能很好地匹配样式特征，并且合成图片与初始内容图片输入到神经网络时在第二层输出能匹配到内容特征。\n",
    "\n",
    "![Neural Style。](../img/neural-style.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import gluonbook as gb\n",
    "from mxnet import autograd, gluon, image, nd\n",
    "from mxnet.gluon import model_zoo, nn\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据\n",
    "\n",
    "我们分别读取样式和内容图片。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [],
   "source": [
    "style_img = image.imread('../img/autumn_oak.jpg')\n",
    "gb.plt.imshow(style_img.asnumpy());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [],
   "source": [
    "content_img = image.imread('../img/rainier.jpg')\n",
    "gb.plt.imshow(content_img.asnumpy());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后定义预处理和后处理函数。预处理函数将原始图片进行归一化并转换成卷积网络接受的输入格式，后处理函数则还原成能展示的图片格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "rgb_mean = nd.array([0.485, 0.456, 0.406])\n",
    "rgb_std = nd.array([0.229, 0.224, 0.225])\n",
    "\n",
    "def preprocess(img, image_shape):\n",
    "    img = image.imresize(img, *image_shape)\n",
    "    img = (img.astype('float32') / 255 - rgb_mean) / rgb_std\n",
    "    return img.transpose((2, 0, 1)).expand_dims(axis=0)\n",
    "\n",
    "def postprocess(img):\n",
    "    img = img[0].as_in_context(rgb_std.context)\n",
    "    return (img.transpose((1, 2, 0)) * rgb_std + rgb_mean).clip(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 抽取特征\n",
    "\n",
    "我们使用原论文 [1] 使用的 VGG 19 模型，并下载在 Imagenet 上训练好的权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "pretrained_net = model_zoo.vision.vgg19(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们知道 VGG 使用了五个卷积块来构建网络，块之间使用最大池化层来做间隔（参考 [“使用重复元素的网络（VGG）”](../chapter_convolutional-neural-networks/vgg.md) 小节）。[1] 中使用每个卷积块的第一个卷积层输出来匹配样式（称之为样式层），和第四块中的最后一个卷积层来匹配内容（称之为内容层）。我们可以打印 `pretrained_net` 来获取这些层的具体位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    }
   },
   "outputs": [],
   "source": [
    "style_layers = [0, 5, 10, 19, 28]\n",
    "content_layers = [25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，样式层和内容层有多种选取方法。通常越靠近输入层越容易匹配内容和样式的细节信息，越靠近输出则越倾向于语义的内容和全局的样式。这里我们选取比较靠后的内容层来避免合成图片过于保留内容图片细节，使用多个位置的样式层来匹配局部和全局样式。\n",
    "\n",
    "下面构建一个新的网络使其只保留我们需要预留的层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    }
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "for i in range(max(content_layers + style_layers) + 1):\n",
    "    net.add(pretrained_net.features[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定输入 `x`，简单使用 `net(x)` 只能拿到最后的输出，而这里我们还需要中间层输出。因此我们我们逐层计算，并保留样式层和内容层的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "14"
    }
   },
   "outputs": [],
   "source": [
    "def extract_features(x, content_layers, style_layers):\n",
    "    contents = []\n",
    "    styles = []\n",
    "    for i in range(len(net)):\n",
    "        x = net[i](x)\n",
    "        if i in style_layers:\n",
    "            styles.append(x)\n",
    "        if i in content_layers:\n",
    "            contents.append(x)\n",
    "    return contents, styles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后我们定义函数分别对内容图片和样式图片抽取对应的特征。因为在训练时我们不修改网络的权重，所以我们可以在训练开始之前提取抽所要的特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contents(image_shape, ctx):\n",
    "    content_x = preprocess(content_img, image_shape).copyto(ctx)\n",
    "    content_y, _ = extract_features(content_x, content_layers, style_layers)\n",
    "    return content_x, content_y\n",
    "\n",
    "def get_styles(image_shape, ctx):\n",
    "    style_x = preprocess(style_img, image_shape).copyto(ctx)\n",
    "    _, style_y = extract_features(style_x, content_layers, style_layers)\n",
    "    return style_x, style_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数\n",
    "\n",
    "在训练时，我们需要定义如何比较合成图片和内容图片的内容层输出（内容损失函数），以及比较和样式图片的样式层输出（样式损失函数）。内容损失函数可以使用回归用的均方误差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(y_hat, y):\n",
    "    return (y_hat - y).square().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于样式，我们可以简单将它看成是像素点在每个通道的统计分布。例如要匹配两张图片的样式，我们可以匹配这两张图片在 RGB 这三个通道上的直方图。更一般的，假设卷积层的输出格式是 $c \\times h \\times w$，既（通道，高，宽）。那么我们可以把它变形成 $c \\times hw$ 的二维数组，并将它看成是一个维度为 $c$ 的随机变量采样到的 $hw$ 个点。所谓的样式匹配就是使得两个 $c$ 维随机变量统计分布一致。\n",
    "\n",
    "匹配统计分布常用的做法是冲量匹配，就是说使得他们有一样的均值，协方差，和其他高维的冲量。为了计算简单起见，我们只匹配二阶信息，即协方差。下面定义如何计算协方差矩阵，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    }
   },
   "outputs": [],
   "source": [
    "def gram(x):\n",
    "    c, n = x.shape[1], x.size // x.shape[1]\n",
    "    y = x.reshape((c, n))\n",
    "    return nd.dot(y, y.T) / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和对应的损失函数，这里假设样式图片的样式特征协方差已经预先计算好了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_loss(y_hat, gram_y):\n",
    "    return (gram(y_hat) - gram_y).square().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们使用靠近输出层的神经层输出来匹配时，经常可以观察到学到的合成图片里面有大量高频噪音，即有特别亮或者暗的颗粒像素。一种常用的降噪方法是总变差降噪（total variation denoising）。假设 $x_{i,j}$ 表示像素 $(i,j)$ 的值，总变差损失使得邻近的像素值相似：\n",
    "\n",
    "$$\n",
    "\\sum_{i,j} |x_{i,j} - x_{i+1,j}| + |x_{i,j} - x_{i,j+1}|\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tv_loss(y_hat):\n",
    "    return 0.5 * ((y_hat[:,:,1:,:] - y_hat[:,:,:-1,:]).abs().mean() +\n",
    "                  (y_hat[:,:,:,1:] - y_hat[:,:,:,:-1]).abs().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练中我们将上述三个损失函数加权求和。通过调整权重值我们可以控制学到的图片是否保留更多样式，更多内容，还是更加干净。此外注意到样式层里有五个神经层，我们对靠近输入的有较少的通道数的层给予比较大的权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    }
   },
   "outputs": [],
   "source": [
    "style_channels = [net[l].weight.shape[0] for l in style_layers]\n",
    "style_weights = [1e4 / c**2 for c in style_channels]\n",
    "content_weights = [1]\n",
    "tv_weight = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练\n",
    "\n",
    "这里的训练跟前面章节的主要不同在于我们只对输入 `x` 进行更新。此外我们将 `x` 的梯度除以了它的绝对平均值来降低对学习率的敏感度，而且每隔一定的批量我们减小一次学习率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "18"
    }
   },
   "outputs": [],
   "source": [
    "def train(x, content_y, style_y, ctx, lr, max_epochs, lr_decay_epoch):\n",
    "    x = x.as_in_context(ctx)\n",
    "    x.attach_grad()\n",
    "    style_y_gram = [gram(y) for y in style_y]\n",
    "    for i in range(1, max_epochs + 1):\n",
    "        tic = time.time()\n",
    "        with autograd.record():\n",
    "            # 对 x 抽取样式和内容特征。\n",
    "            content_y_hat, style_y_hat = extract_features(\n",
    "                x, content_layers, style_layers)\n",
    "            # 分别计算内容、样式和噪音损失。\n",
    "            content_L = [w * content_loss(y_hat, y) for w, y_hat, y in zip(\n",
    "                content_weights, content_y_hat, content_y)]\n",
    "            style_L = [w * style_loss(y_hat, y) for w, y_hat, y in zip(\n",
    "                style_weights, style_y_hat, style_y_gram)]\n",
    "            tv_L = tv_weight * tv_loss(x)\n",
    "            # 对所有损失求和。\n",
    "            l = nd.add_n(*style_L) + nd.add_n(*content_L) + tv_L\n",
    "        l.backward()\n",
    "        # 对 x 的梯度除去绝对均值使得数值更加稳定，并更新 x\n",
    "        x.grad[:] /= x.grad.abs().mean() + 1e-8\n",
    "        x[:] -= lr * x.grad\n",
    "        # 如果不加的话会导致每 50 轮迭代才同步一次，可能导致过大内存使用。\n",
    "        nd.waitall()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print('batch %3d: content %.2f, style %.2f, '\n",
    "                  'TV %.2f, %.1f sec per batch'\n",
    "                  % (i, nd.add_n(*content_L).asscalar(),\n",
    "                     nd.add_n(*style_L).asscalar(), tv_L.asscalar(),\n",
    "                     time.time() - tic))\n",
    "            \n",
    "        if i % lr_decay_epoch == 0:\n",
    "            lr *= 0.1\n",
    "            print('change lr to %.1e' % lr)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们可以真正开始训练了。首先我们将图片调整到高为 300 宽 200 来进行训练，这样使得训练更加快速。合成图片的初始值设成了内容图片，使得初始值能尽可能接近训练输出来加速收敛。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "19"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch  50: content 34.66, style 504.35, TV 4.76, 0.1 sec per batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 100: content 33.17, style 533.62, TV 5.33, 0.1 sec per batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 150: content 35.22, style 848.21, TV 5.64, 0.1 sec per batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 200: content 36.26, style 737.48, TV 5.77, 0.1 sec per batch\n",
      "change lr to 1.0e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 250: content 20.05, style 28.10, TV 5.55, 0.1 sec per batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 300: content 17.74, style 21.74, TV 5.49, 0.1 sec per batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 350: content 16.59, style 18.81, TV 5.44, 0.1 sec per batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 400: content 15.45, style 18.79, TV 5.39, 0.1 sec per batch\n",
      "change lr to 1.0e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 450: content 14.64, style 13.05, TV 5.36, 0.1 sec per batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 500: content 14.13, style 12.32, TV 5.34, 0.1 sec per batch\n"
     ]
    }
   ],
   "source": [
    "image_shape = (300, 200)\n",
    "ctx = gb.try_gpu()\n",
    "\n",
    "net.collect_params().reset_ctx(ctx)\n",
    "content_x, content_y = get_contents(image_shape, ctx)\n",
    "style_x, style_y = get_styles(image_shape, ctx)\n",
    "\n",
    "x = content_x\n",
    "y = train(x, content_y, style_y, ctx, 0.1, 500, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为使用了内容图片作为初始值，所以一开始内容误差远小于样式误差。随着迭代的进行样式误差迅速减少，最终它们值在相近的范围。下面我们将训练好的合成图片保存下来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb.plt.imsave('neural-style-1.png', postprocess(y).asnumpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![300 x 200 尺寸的合成图片。](./neural-style-1.png)\n",
    "\n",
    "可以看到合成图片保留了样式图片的风景物体，同时借鉴了样式图片的色彩。由于图片尺寸较小，所以细节上比较模糊。下面我们在更大的 `1200 x 800` 的尺寸上训练，希望可以得到更加清晰的合成图片。为了加速收敛，我们将训练到的合成图片高宽放大 3 倍来作为初始值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "20"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch  50: content 22.48, style 798.46, TV 3.70, 0.9 sec per batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 100: content 23.94, style 604.76, TV 4.19, 1.0 sec per batch\n",
      "change lr to 1.0e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 150: content 18.34, style 53.68, TV 3.40, 1.0 sec per batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 200: content 17.44, style 17.50, TV 3.25, 1.0 sec per batch\n",
      "change lr to 1.0e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 250: content 15.42, style 12.17, TV 3.17, 1.0 sec per batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 300: content 14.66, style 10.88, TV 3.13, 1.0 sec per batch\n",
      "change lr to 1.0e-04\n"
     ]
    }
   ],
   "source": [
    "image_shape = (1200, 800)\n",
    "\n",
    "content_x, content_y = get_contents(image_shape, ctx)\n",
    "style_x, style_y = get_styles(image_shape, ctx)\n",
    "\n",
    "x = preprocess(postprocess(y) * 255, image_shape)\n",
    "z = train(x, content_y, style_y, ctx, 0.1, 300, 100)\n",
    "\n",
    "gb.plt.imsave('neural-style-2.png', postprocess(z).asnumpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到这一次由于初始值离最终输出更近使得收敛更加迅速。但同时由于图片尺寸更大，每一次迭代需要花费更多的时间和内存。\n",
    "\n",
    "![1200 x 800 尺寸的合成图片。](./neural-style-2.png)\n",
    "\n",
    "从训练得到的图片可以看到它保留了更多的细节，里面不仅有大块的类似样式图片的油画色彩块，色彩块里面也有细微的纹理。\n",
    "\n",
    "## 小结\n",
    "\n",
    "* 通过匹配神经网络的中间层输出可以有效的融合不同图片的内容和样式。\n",
    "\n",
    "## 练习\n",
    "\n",
    "* 选择不同的内容和样式层。\n",
    "* 使用不同的损失权重来得到更偏向内容或样式或平滑的输出。\n",
    "* 一个得到更加干净的合成图的办法是使用更大的尺寸。\n",
    "* 换别的样式和内容图片试试。\n",
    "\n",
    "## 扫码直达 [ 讨论区 ](https://discuss.gluon.ai/t/topic/3273)\n",
    "\n",
    "![](../img/qr_neural-style.svg)\n",
    "\n",
    "## 参考文献\n",
    "\n",
    "[1] Gatys, Leon A., Alexander S. Ecker, and Matthias Bethge. \"Image style transfer using convolutional neural networks.\" CVPR. 2016."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}